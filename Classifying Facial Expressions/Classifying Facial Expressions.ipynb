{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d5b302",
   "metadata": {},
   "source": [
    "# Classifying Facial Expressions\n",
    "## Author: Ahria Dominguez\n",
    "### Last Updated: 8/10/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c07da1",
   "metadata": {},
   "source": [
    "In this project, we will examine data from a Kaggle dataset titled _FER-2013_ (https://www.kaggle.com/datasets/msambare/fer2013). We will determine if images of facial expressions can be classified into one of seven different emotional categories using convolutional neural networks (CNNs).\n",
    "\n",
    "The data used in these models consist of:\n",
    "- Original, non-cleaned data:\n",
    "    - Training Images\n",
    "        - Angry: 3995\n",
    "        - Disgust: 436\n",
    "        - Fear: 4097\n",
    "        - Happy: 7215\n",
    "        - Neutral: 4965\n",
    "        - Sad: 4830\n",
    "        - Surprise: 3171\n",
    "    - Testing Images\n",
    "        - Angry: 958\n",
    "        - Disgust: 111\n",
    "        - Fear: 1024\n",
    "        - Happy: 1774\n",
    "        - Neutral: 1233\n",
    "        - Sad: 1247\n",
    "        - Surprise:831 \n",
    "- Cleaned data (all non-facial expression photos were removed):\n",
    "    - Training Images\n",
    "        - Angry: 3981\n",
    "        - Disgust: 435\n",
    "        - Fear: 4089\n",
    "        - Happy: 7203\n",
    "        - Neutral: 4952\n",
    "        - Sad: 4822\n",
    "        - Surprise: 3163 \n",
    "    - Testing Images\n",
    "        - Angry: 956\n",
    "        - Disgust: 111\n",
    "        - Fear: 1023\n",
    "        - Happy: 1772\n",
    "        - Neutral: 1228\n",
    "        - Sad: 1245\n",
    "        - Surprise: 829 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745cd1bc",
   "metadata": {},
   "source": [
    "##### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14c6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary libraries to process, work with, classify the images,\n",
    "# and plot results.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, \\\n",
    "                                    BatchNormalization\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb089a",
   "metadata": {},
   "source": [
    "##### Running Models on the Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a6b3a",
   "metadata": {},
   "source": [
    "###### Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32dbc7",
   "metadata": {},
   "source": [
    "In order to compare how cleaning the dataset impacts the results of the model, I have to run the models on the original, untouched dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837135da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets variables to the name of the folder the data is saved to.\n",
    "orig_training = 'train_original'\n",
    "orig_testing = 'test_original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7509f0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Initializes the training image data generator and\n",
    "# rescales the images by dividing the pixels by 255.\n",
    "train_datagenerator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Initializes the testing image data generator, and also rescales the images\n",
    "# by dividing the pixels by 255, just like the training data.\n",
    "test_datagenerator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Initializes the training generator using the previously defined \n",
    "# 'train_datagenerator' variable to create batches of data from the\n",
    "# orig_training directory.\n",
    "train_generator = train_datagenerator.flow_from_directory(\n",
    "    orig_training,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions.\n",
    "\n",
    "# Initializes the testing generator using the previously defined\n",
    "# 'test_datagenerator' variable to create batches of data from the\n",
    "# orig_testing directory.\n",
    "test_generator = test_datagenerator.flow_from_directory(\n",
    "    orig_testing,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bfacd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 19:57:04.140901: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-08-09 19:57:04.140922: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-08-09 19:57:04.140927: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-08-09 19:57:04.140963: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-08-09 19:57:04.140979: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# The CNN model of 'Sequential' allows for us to add layers to the model\n",
    "# one at a time. This initializes the model.\n",
    "cnn = Sequential()\n",
    "\n",
    "# Adds a 2D convolutional layer with 32 filters, all 3x3 pixels, sets the \n",
    "# activation method as ReLU to make all negative values zero, and defines\n",
    "# the input shape as 48x48 pixels.\n",
    "cnn.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', \n",
    "                 input_shape = (48, 48, 1)))\n",
    "# Adds a max pooling layer with a size of 2x2, which helps mitigate overfitting.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 64 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 128 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattens the data into a 1D vector.\n",
    "cnn.add(Flatten())\n",
    "# Adds a fully connected layer with 128 neurons.\n",
    "cnn.add(Dense(128, activation = 'relu'))\n",
    "# Adds a dropout layer to prevent overfitting. 50% of the inputs are changed\n",
    "# to zero during this training.\n",
    "cnn.add(Dropout(0.5))\n",
    "# Adds an output layer with 7 neurons (1 for each class) and converts the output\n",
    "# into probabilities (softmax). \n",
    "cnn.add(Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a43be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the model ready for use with the photos, using the Adam optimizer (a good\n",
    "# optimizer to adjust the learning rate during training), categorical crossentropy\n",
    "# (good for multi-class classification problems), and the accuracy metric (to \n",
    "# evaluate how well the model does). \n",
    "cnn.compile(optimizer = Adam(), loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d926c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/897 [..............................] - ETA: 6:13 - loss: 1.9528 - accuracy: 0.1875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 19:57:04.568480: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897/897 [==============================] - 12s 13ms/step - loss: 1.7368 - accuracy: 0.2957 - val_loss: 1.5786 - val_accuracy: 0.3948\n",
      "Epoch 2/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.5256 - accuracy: 0.4137 - val_loss: 1.4164 - val_accuracy: 0.4595\n",
      "Epoch 3/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.4149 - accuracy: 0.4649 - val_loss: 1.3370 - val_accuracy: 0.4948\n",
      "Epoch 4/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.3697 - accuracy: 0.4812 - val_loss: 1.3059 - val_accuracy: 0.5098\n",
      "Epoch 5/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.3441 - accuracy: 0.4978 - val_loss: 1.2894 - val_accuracy: 0.5071\n",
      "Epoch 6/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.3384 - accuracy: 0.4996 - val_loss: 1.3225 - val_accuracy: 0.4919\n",
      "Epoch 7/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.3445 - accuracy: 0.5057 - val_loss: 1.2723 - val_accuracy: 0.5215\n",
      "Epoch 8/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.3785 - accuracy: 0.4987 - val_loss: 1.3480 - val_accuracy: 0.4873\n",
      "Epoch 9/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.4002 - accuracy: 0.4967 - val_loss: 1.4348 - val_accuracy: 0.4692\n",
      "Epoch 10/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4316 - accuracy: 0.4922 - val_loss: 1.3302 - val_accuracy: 0.5114\n",
      "Epoch 11/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 1.4896 - accuracy: 0.4881 - val_loss: 1.3949 - val_accuracy: 0.4842\n",
      "Epoch 12/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.5368 - accuracy: 0.4877 - val_loss: 1.4392 - val_accuracy: 0.5177\n",
      "Epoch 13/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.6000 - accuracy: 0.4845 - val_loss: 1.4134 - val_accuracy: 0.4957\n",
      "Epoch 14/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 1.7269 - accuracy: 0.4722 - val_loss: 1.8135 - val_accuracy: 0.4565\n",
      "Epoch 15/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.8232 - accuracy: 0.4723 - val_loss: 2.1832 - val_accuracy: 0.4258\n",
      "Epoch 16/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 2.0193 - accuracy: 0.4650 - val_loss: 1.7585 - val_accuracy: 0.4736\n",
      "Epoch 17/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 2.1331 - accuracy: 0.4659 - val_loss: 1.5602 - val_accuracy: 0.4855\n",
      "Epoch 18/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 2.2765 - accuracy: 0.4598 - val_loss: 2.0143 - val_accuracy: 0.4706\n",
      "Epoch 19/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 2.5078 - accuracy: 0.4561 - val_loss: 3.1301 - val_accuracy: 0.3890\n",
      "Epoch 20/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 2.7826 - accuracy: 0.4575 - val_loss: 2.9550 - val_accuracy: 0.4234\n",
      "Epoch 21/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 3.2331 - accuracy: 0.4478 - val_loss: 2.7918 - val_accuracy: 0.4289\n",
      "Epoch 22/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 3.5952 - accuracy: 0.4510 - val_loss: 2.7056 - val_accuracy: 0.4533\n",
      "Epoch 23/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 4.0891 - accuracy: 0.4458 - val_loss: 4.3348 - val_accuracy: 0.4662\n",
      "Epoch 24/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 4.7718 - accuracy: 0.4441 - val_loss: 3.8346 - val_accuracy: 0.4308\n",
      "Epoch 25/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 5.1471 - accuracy: 0.4443 - val_loss: 3.3434 - val_accuracy: 0.4860\n",
      "Epoch 26/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 6.0625 - accuracy: 0.4435 - val_loss: 3.3793 - val_accuracy: 0.5052\n",
      "Epoch 27/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 6.7597 - accuracy: 0.4454 - val_loss: 5.3732 - val_accuracy: 0.4986\n",
      "Epoch 28/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 7.5632 - accuracy: 0.4469 - val_loss: 9.1305 - val_accuracy: 0.3775\n",
      "Epoch 29/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 8.5608 - accuracy: 0.4436 - val_loss: 5.2335 - val_accuracy: 0.5007\n",
      "Epoch 30/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 10.1776 - accuracy: 0.4428 - val_loss: 7.1516 - val_accuracy: 0.4321\n",
      "Epoch 31/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 11.0945 - accuracy: 0.4442 - val_loss: 8.4378 - val_accuracy: 0.4615\n",
      "Epoch 32/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 12.8880 - accuracy: 0.4422 - val_loss: 8.2257 - val_accuracy: 0.4953\n",
      "Epoch 33/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 14.2005 - accuracy: 0.4427 - val_loss: 19.9660 - val_accuracy: 0.3938\n",
      "Epoch 34/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 16.2152 - accuracy: 0.4417 - val_loss: 13.4129 - val_accuracy: 0.4319\n",
      "Epoch 35/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 19.2951 - accuracy: 0.4418 - val_loss: 13.2298 - val_accuracy: 0.4760\n",
      "Epoch 36/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 20.2246 - accuracy: 0.4413 - val_loss: 16.6845 - val_accuracy: 0.3737\n",
      "Epoch 37/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 21.7991 - accuracy: 0.4461 - val_loss: 19.5240 - val_accuracy: 0.4842\n",
      "Epoch 38/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 25.2143 - accuracy: 0.4422 - val_loss: 22.6724 - val_accuracy: 0.4464\n",
      "Epoch 39/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 27.5783 - accuracy: 0.4423 - val_loss: 21.6322 - val_accuracy: 0.4708\n",
      "Epoch 40/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 30.4018 - accuracy: 0.4468 - val_loss: 25.2782 - val_accuracy: 0.4570\n",
      "Epoch 41/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 32.2041 - accuracy: 0.4445 - val_loss: 29.9514 - val_accuracy: 0.4707\n",
      "Epoch 42/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 36.6432 - accuracy: 0.4452 - val_loss: 35.6917 - val_accuracy: 0.4222\n",
      "Epoch 43/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 41.4483 - accuracy: 0.4457 - val_loss: 32.1115 - val_accuracy: 0.4468\n",
      "Epoch 44/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 43.9798 - accuracy: 0.4439 - val_loss: 33.4452 - val_accuracy: 0.3859\n",
      "Epoch 45/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 46.2446 - accuracy: 0.4488 - val_loss: 46.9464 - val_accuracy: 0.4675\n",
      "Epoch 46/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 52.2954 - accuracy: 0.4487 - val_loss: 56.1616 - val_accuracy: 0.4067\n",
      "Epoch 47/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 55.7199 - accuracy: 0.4449 - val_loss: 55.3034 - val_accuracy: 0.4016\n",
      "Epoch 48/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 59.0033 - accuracy: 0.4466 - val_loss: 40.1980 - val_accuracy: 0.4905\n",
      "Epoch 49/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 63.7873 - accuracy: 0.4467 - val_loss: 41.9850 - val_accuracy: 0.4883\n",
      "Epoch 50/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 73.4707 - accuracy: 0.4440 - val_loss: 59.7746 - val_accuracy: 0.4375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x173d63490>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fits the model to the training and validation data, running through the epochs.\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    # Tells the model to divide the number of samples by the batch size of the\n",
    "    # model to calculate how many batches of data will be processed for each epoch.\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size,\n",
    "    # Specifies the number of epochs the model will go through. We set it to\n",
    "    # 50, so the training dataset will go through the model 50 times.\n",
    "    epochs = 50,\n",
    "    # Initializes the validation data the model will use to test its efficiency.\n",
    "    validation_data = test_generator,\n",
    "    # Calculates how many batches of data will be processed for each validation\n",
    "    # epoch. \n",
    "    validation_steps = test_generator.samples // test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd8141b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 1s 6ms/step - loss: 59.7490 - accuracy: 0.4377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[59.748992919921875, 0.43772637844085693]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the model on the test data.\n",
    "cnn.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b072871e",
   "metadata": {},
   "source": [
    "It appears the default model (no extra parameters added) using the original dataset had an accuracy of 43.77% and a loss value of 59.75. Let's try changing up some parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b139ad27",
   "metadata": {},
   "source": [
    "###### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e49e906c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Almost everything will be the same as the first model, except we will\n",
    "# add a parameter of 'shear_range' to the 'train_datagenerator' variable.\n",
    "\n",
    "# Initializes the training image data generator.\n",
    "train_datagenerator = ImageDataGenerator(\n",
    "    rescale = 1./255, # Rescales the images by dividing the pixels by 255.\n",
    "    shear_range = 0.2) # Applies a shear transformation to the images (slanting\n",
    "                       # the image by up to 20%).\n",
    "\n",
    "# Initializes the testing image data generator, and also rescales the images\n",
    "# by dividing the pixels by 255, just like the training data.\n",
    "test_datagenerator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Initializes the training generator using the previously defined \n",
    "# 'train_datagenerator' variable to create batches of data from the\n",
    "# orig_training directory.\n",
    "train_generator = train_datagenerator.flow_from_directory(\n",
    "    orig_training,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions.\n",
    "\n",
    "# Initializes the testing generator using the previously defined\n",
    "# 'test_datagenerator' variable to create batches of data from the\n",
    "# orig_testing directory.\n",
    "test_generator = test_datagenerator.flow_from_directory(\n",
    "    orig_testing,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "195c0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the Sequential model.\n",
    "cnn = Sequential()\n",
    "\n",
    "# Adds a 2D convolutional layer with 32 filters, all 3x3 pixels, sets the \n",
    "# activation method as ReLU to make all negative values zero, and defines\n",
    "# the input shape as 48x48 pixels.\n",
    "cnn.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', \n",
    "                 input_shape = (48, 48, 1)))\n",
    "# Adds a max pooling layer with a size of 2x2, which helps mitigate overfitting.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 64 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 128 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattens the data into a 1D vector.\n",
    "cnn.add(Flatten())\n",
    "# Adds a fully connected layer with 128 neurons.\n",
    "cnn.add(Dense(128, activation = 'relu'))\n",
    "# Adds a dropout layer to prevent overfitting. 50% of the inputs are changed\n",
    "# to zero during this training.\n",
    "cnn.add(Dropout(0.5))\n",
    "# Adds an output layer with 7 neurons (1 for each class) and converts the output\n",
    "# into probabilities (softmax). \n",
    "cnn.add(Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab8acaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the model ready for use with the photos, using the same parameters\n",
    "# as the model before. \n",
    "cnn.compile(optimizer = Adam(), loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc62af9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 1.7347 - accuracy: 0.3040 - val_loss: 1.5623 - val_accuracy: 0.4054\n",
      "Epoch 2/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.5154 - accuracy: 0.4169 - val_loss: 1.4102 - val_accuracy: 0.4583\n",
      "Epoch 3/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4106 - accuracy: 0.4655 - val_loss: 1.3327 - val_accuracy: 0.4975\n",
      "Epoch 4/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.3492 - accuracy: 0.4863 - val_loss: 1.2841 - val_accuracy: 0.5098\n",
      "Epoch 5/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 1.3068 - accuracy: 0.5062 - val_loss: 1.3006 - val_accuracy: 0.5166\n",
      "Epoch 6/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 1.2877 - accuracy: 0.5168 - val_loss: 1.2537 - val_accuracy: 0.5269\n",
      "Epoch 7/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 1.2810 - accuracy: 0.5244 - val_loss: 1.2821 - val_accuracy: 0.5126\n",
      "Epoch 8/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.2668 - accuracy: 0.5303 - val_loss: 1.2381 - val_accuracy: 0.5331\n",
      "Epoch 9/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 1.2683 - accuracy: 0.5330 - val_loss: 1.3523 - val_accuracy: 0.5038\n",
      "Epoch 10/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.2711 - accuracy: 0.5324 - val_loss: 1.2535 - val_accuracy: 0.5356\n",
      "Epoch 11/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.2777 - accuracy: 0.5334 - val_loss: 1.3123 - val_accuracy: 0.5283\n",
      "Epoch 12/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.2914 - accuracy: 0.5369 - val_loss: 1.3129 - val_accuracy: 0.5243\n",
      "Epoch 13/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.3067 - accuracy: 0.5317 - val_loss: 1.3658 - val_accuracy: 0.5278\n",
      "Epoch 14/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.3533 - accuracy: 0.5291 - val_loss: 1.3956 - val_accuracy: 0.5269\n",
      "Epoch 15/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.3726 - accuracy: 0.5304 - val_loss: 1.2532 - val_accuracy: 0.5512\n",
      "Epoch 16/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.4321 - accuracy: 0.5245 - val_loss: 1.3106 - val_accuracy: 0.5480\n",
      "Epoch 17/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4417 - accuracy: 0.5288 - val_loss: 1.5363 - val_accuracy: 0.4784\n",
      "Epoch 18/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.5120 - accuracy: 0.5215 - val_loss: 1.4186 - val_accuracy: 0.5253\n",
      "Epoch 19/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.5654 - accuracy: 0.5194 - val_loss: 1.7040 - val_accuracy: 0.5084\n",
      "Epoch 20/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.5886 - accuracy: 0.5227 - val_loss: 1.8231 - val_accuracy: 0.4574\n",
      "Epoch 21/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.6978 - accuracy: 0.5177 - val_loss: 1.7255 - val_accuracy: 0.5227\n",
      "Epoch 22/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.7999 - accuracy: 0.5176 - val_loss: 1.6343 - val_accuracy: 0.5416\n",
      "Epoch 23/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.8971 - accuracy: 0.5164 - val_loss: 1.7504 - val_accuracy: 0.5056\n",
      "Epoch 24/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 2.0199 - accuracy: 0.5108 - val_loss: 2.3728 - val_accuracy: 0.4619\n",
      "Epoch 25/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.1381 - accuracy: 0.5090 - val_loss: 2.6031 - val_accuracy: 0.4563\n",
      "Epoch 26/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 2.2833 - accuracy: 0.5063 - val_loss: 1.9984 - val_accuracy: 0.5303\n",
      "Epoch 27/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.4882 - accuracy: 0.5058 - val_loss: 2.1731 - val_accuracy: 0.5063\n",
      "Epoch 28/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.7373 - accuracy: 0.5023 - val_loss: 2.2258 - val_accuracy: 0.5321\n",
      "Epoch 29/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.9113 - accuracy: 0.5022 - val_loss: 2.8205 - val_accuracy: 0.4934\n",
      "Epoch 30/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.9970 - accuracy: 0.5103 - val_loss: 3.4128 - val_accuracy: 0.5273\n",
      "Epoch 31/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 3.4387 - accuracy: 0.5032 - val_loss: 2.7789 - val_accuracy: 0.5326\n",
      "Epoch 32/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 3.6787 - accuracy: 0.5042 - val_loss: 3.5690 - val_accuracy: 0.4813\n",
      "Epoch 33/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 3.9992 - accuracy: 0.5026 - val_loss: 5.4271 - val_accuracy: 0.4749\n",
      "Epoch 34/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 4.3956 - accuracy: 0.4993 - val_loss: 3.4291 - val_accuracy: 0.5264\n",
      "Epoch 35/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 4.8997 - accuracy: 0.5018 - val_loss: 4.8709 - val_accuracy: 0.4886\n",
      "Epoch 36/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 5.5171 - accuracy: 0.4999 - val_loss: 4.6054 - val_accuracy: 0.4976\n",
      "Epoch 37/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 5.3856 - accuracy: 0.5065 - val_loss: 3.8924 - val_accuracy: 0.5282\n",
      "Epoch 38/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 6.1411 - accuracy: 0.5060 - val_loss: 5.1473 - val_accuracy: 0.5174\n",
      "Epoch 39/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 6.3776 - accuracy: 0.5100 - val_loss: 6.6640 - val_accuracy: 0.4940\n",
      "Epoch 40/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 7.7333 - accuracy: 0.5010 - val_loss: 9.9038 - val_accuracy: 0.5053\n",
      "Epoch 41/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 7.9698 - accuracy: 0.5041 - val_loss: 6.3420 - val_accuracy: 0.5346\n",
      "Epoch 42/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 8.7411 - accuracy: 0.5055 - val_loss: 6.9939 - val_accuracy: 0.5138\n",
      "Epoch 43/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 9.4770 - accuracy: 0.5065 - val_loss: 12.6551 - val_accuracy: 0.4516\n",
      "Epoch 44/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 10.5305 - accuracy: 0.5065 - val_loss: 8.3440 - val_accuracy: 0.5088\n",
      "Epoch 45/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 11.2656 - accuracy: 0.5078 - val_loss: 11.4659 - val_accuracy: 0.5100\n",
      "Epoch 46/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 11.7794 - accuracy: 0.5138 - val_loss: 11.1352 - val_accuracy: 0.4840\n",
      "Epoch 47/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 13.3057 - accuracy: 0.5074 - val_loss: 12.3148 - val_accuracy: 0.4985\n",
      "Epoch 48/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 14.0318 - accuracy: 0.5078 - val_loss: 12.8439 - val_accuracy: 0.4880\n",
      "Epoch 49/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 14.9159 - accuracy: 0.5119 - val_loss: 12.1743 - val_accuracy: 0.5266\n",
      "Epoch 50/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 16.5408 - accuracy: 0.5067 - val_loss: 19.5281 - val_accuracy: 0.4912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x302b565d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fits the model to the training and validation data, running through the epochs.\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    # Tells the model to divide the number of samples by the batch size of the\n",
    "    # model to calculate how many batches of data will be processed for each epoch.\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size,\n",
    "    # Specifies the number of epochs the model will go through. We set it to\n",
    "    # 50, so the training dataset will go through the model 50 times.\n",
    "    epochs = 50,\n",
    "    # Initializes the validation data the model will use to test its efficiency.\n",
    "    validation_data = test_generator,\n",
    "    # Calculates how many batches of data will be processed for each validation\n",
    "    # epoch. \n",
    "    validation_steps = test_generator.samples // test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c574507b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 1s 6ms/step - loss: 19.5455 - accuracy: 0.4909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19.54549789428711, 0.4909445643424988]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the model on the test data.\n",
    "cnn.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc5d0a",
   "metadata": {},
   "source": [
    "Adding the parameter of 'shear_range' in this second model seemed to help a bit. The model's accuracy was 49.09%, and the loss value was 19.55. Let's try adding another parameter to see if it gets better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520f8606",
   "metadata": {},
   "source": [
    "###### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "844947bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Almost everything will be the same as the second model, except we will\n",
    "# add a parameter of 'zoom_range' to the 'train_datagenerator' variable.\n",
    "\n",
    "# Initializes the training image data generator.\n",
    "train_datagenerator = ImageDataGenerator(\n",
    "    rescale = 1./255, # Rescales the images by dividing the pixels by 255.\n",
    "    shear_range = 0.2, # Slants the image by up to 20%.\n",
    "    zoom_range = 0.2)  # Zooms into our away from the image by up to 20%.\n",
    "\n",
    "# Initializes the testing image data generator, and also rescales the images\n",
    "# by dividing the pixels by 255, just like the training data.\n",
    "test_datagenerator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Initializes the training generator using the previously defined \n",
    "# 'train_datagenerator' variable to create batches of data from the\n",
    "# orig_training directory.\n",
    "train_generator = train_datagenerator.flow_from_directory(\n",
    "    orig_training,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions.\n",
    "\n",
    "# Initializes the testing generator using the previously defined\n",
    "# 'test_datagenerator' variable to create batches of data from the\n",
    "# orig_testing directory.\n",
    "test_generator = test_datagenerator.flow_from_directory(\n",
    "    orig_testing,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0edb4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the Sequential model.\n",
    "cnn = Sequential()\n",
    "\n",
    "# Adds a 2D convolutional layer with 32 filters, all 3x3 pixels, sets the \n",
    "# activation method as ReLU to make all negative values zero, and defines\n",
    "# the input shape as 48x48 pixels.\n",
    "cnn.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', \n",
    "                 input_shape = (48, 48, 1)))\n",
    "# Adds a max pooling layer with a size of 2x2, which helps mitigate overfitting.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 64 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 128 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattens the data into a 1D vector.\n",
    "cnn.add(Flatten())\n",
    "# Adds a fully connected layer with 128 neurons.\n",
    "cnn.add(Dense(128, activation = 'relu'))\n",
    "# Adds a dropout layer to prevent overfitting. 50% of the inputs are changed\n",
    "# to zero during this training.\n",
    "cnn.add(Dropout(0.5))\n",
    "# Adds an output layer with 7 neurons (1 for each class) and converts the output\n",
    "# into probabilities (softmax). \n",
    "cnn.add(Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd2e868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the model ready for use with the photos, using the same parameters\n",
    "# as the model before. \n",
    "cnn.compile(optimizer = Adam(), loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b4905ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 1.7759 - accuracy: 0.2703 - val_loss: 1.6570 - val_accuracy: 0.3376\n",
      "Epoch 2/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.6271 - accuracy: 0.3648 - val_loss: 1.4547 - val_accuracy: 0.4453\n",
      "Epoch 3/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.5214 - accuracy: 0.4148 - val_loss: 1.3903 - val_accuracy: 0.4618\n",
      "Epoch 4/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4731 - accuracy: 0.4410 - val_loss: 1.3449 - val_accuracy: 0.4824\n",
      "Epoch 5/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4378 - accuracy: 0.4527 - val_loss: 1.3779 - val_accuracy: 0.4674\n",
      "Epoch 6/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4189 - accuracy: 0.4606 - val_loss: 1.3014 - val_accuracy: 0.5089\n",
      "Epoch 7/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 1.4075 - accuracy: 0.4636 - val_loss: 1.3088 - val_accuracy: 0.5003\n",
      "Epoch 8/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.3954 - accuracy: 0.4753 - val_loss: 1.3136 - val_accuracy: 0.5032\n",
      "Epoch 9/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4045 - accuracy: 0.4739 - val_loss: 1.3054 - val_accuracy: 0.5039\n",
      "Epoch 10/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.4111 - accuracy: 0.4718 - val_loss: 1.3143 - val_accuracy: 0.4932\n",
      "Epoch 11/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4254 - accuracy: 0.4700 - val_loss: 1.3632 - val_accuracy: 0.4923\n",
      "Epoch 12/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4299 - accuracy: 0.4711 - val_loss: 1.2664 - val_accuracy: 0.5239\n",
      "Epoch 13/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4354 - accuracy: 0.4713 - val_loss: 1.3673 - val_accuracy: 0.4999\n",
      "Epoch 14/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4470 - accuracy: 0.4753 - val_loss: 1.3966 - val_accuracy: 0.4897\n",
      "Epoch 15/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4733 - accuracy: 0.4642 - val_loss: 1.3369 - val_accuracy: 0.4954\n",
      "Epoch 16/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4660 - accuracy: 0.4752 - val_loss: 1.3307 - val_accuracy: 0.5074\n",
      "Epoch 17/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4902 - accuracy: 0.4701 - val_loss: 1.2902 - val_accuracy: 0.5229\n",
      "Epoch 18/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.5278 - accuracy: 0.4663 - val_loss: 1.4311 - val_accuracy: 0.4795\n",
      "Epoch 19/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.5884 - accuracy: 0.4589 - val_loss: 1.4078 - val_accuracy: 0.5218\n",
      "Epoch 20/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.5909 - accuracy: 0.4574 - val_loss: 1.3667 - val_accuracy: 0.4919\n",
      "Epoch 21/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.6533 - accuracy: 0.4548 - val_loss: 1.5349 - val_accuracy: 0.4855\n",
      "Epoch 22/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.6570 - accuracy: 0.4582 - val_loss: 1.5073 - val_accuracy: 0.4683\n",
      "Epoch 23/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.7417 - accuracy: 0.4539 - val_loss: 1.5452 - val_accuracy: 0.4902\n",
      "Epoch 24/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.8212 - accuracy: 0.4439 - val_loss: 2.2889 - val_accuracy: 0.3269\n",
      "Epoch 25/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.8746 - accuracy: 0.4436 - val_loss: 1.8844 - val_accuracy: 0.4516\n",
      "Epoch 26/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.9726 - accuracy: 0.4380 - val_loss: 1.5893 - val_accuracy: 0.5102\n",
      "Epoch 27/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 2.0276 - accuracy: 0.4372 - val_loss: 1.7128 - val_accuracy: 0.4785\n",
      "Epoch 28/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 2.1030 - accuracy: 0.4321 - val_loss: 1.9806 - val_accuracy: 0.4396\n",
      "Epoch 29/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.1405 - accuracy: 0.4347 - val_loss: 1.6692 - val_accuracy: 0.5151\n",
      "Epoch 30/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 2.4025 - accuracy: 0.4302 - val_loss: 1.6814 - val_accuracy: 0.5033\n",
      "Epoch 31/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 2.4862 - accuracy: 0.4254 - val_loss: 2.5024 - val_accuracy: 0.4195\n",
      "Epoch 32/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 2.6470 - accuracy: 0.4246 - val_loss: 2.8916 - val_accuracy: 0.4353\n",
      "Epoch 33/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 2.7420 - accuracy: 0.4168 - val_loss: 2.0091 - val_accuracy: 0.4752\n",
      "Epoch 34/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.7505 - accuracy: 0.4251 - val_loss: 2.3005 - val_accuracy: 0.4706\n",
      "Epoch 35/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.8790 - accuracy: 0.4169 - val_loss: 1.8309 - val_accuracy: 0.5091\n",
      "Epoch 36/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 3.0695 - accuracy: 0.4250 - val_loss: 2.8626 - val_accuracy: 0.4287\n",
      "Epoch 37/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 3.2220 - accuracy: 0.4182 - val_loss: 2.1265 - val_accuracy: 0.5114\n",
      "Epoch 38/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 3.4197 - accuracy: 0.4175 - val_loss: 2.2732 - val_accuracy: 0.4936\n",
      "Epoch 39/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 3.6575 - accuracy: 0.4179 - val_loss: 3.1278 - val_accuracy: 0.4992\n",
      "Epoch 40/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 4.2174 - accuracy: 0.4134 - val_loss: 2.4350 - val_accuracy: 0.5081\n",
      "Epoch 41/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 4.2184 - accuracy: 0.4150 - val_loss: 3.0029 - val_accuracy: 0.4763\n",
      "Epoch 42/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 4.5683 - accuracy: 0.4132 - val_loss: 2.8806 - val_accuracy: 0.4986\n",
      "Epoch 43/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 4.6688 - accuracy: 0.4143 - val_loss: 3.1952 - val_accuracy: 0.4595\n",
      "Epoch 44/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 5.2512 - accuracy: 0.4116 - val_loss: 3.6832 - val_accuracy: 0.4752\n",
      "Epoch 45/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 5.3021 - accuracy: 0.4113 - val_loss: 5.6849 - val_accuracy: 0.3870\n",
      "Epoch 46/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 6.1533 - accuracy: 0.4109 - val_loss: 3.8580 - val_accuracy: 0.4860\n",
      "Epoch 47/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 6.8779 - accuracy: 0.4074 - val_loss: 7.0468 - val_accuracy: 0.4381\n",
      "Epoch 48/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 6.9095 - accuracy: 0.4149 - val_loss: 4.6770 - val_accuracy: 0.4908\n",
      "Epoch 49/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 7.4640 - accuracy: 0.4142 - val_loss: 6.8445 - val_accuracy: 0.4858\n",
      "Epoch 50/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 7.6966 - accuracy: 0.4086 - val_loss: 6.9517 - val_accuracy: 0.4819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3212f88d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fits the model to the training and validation data, running through the epochs.\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    # Tells the model to divide the number of samples by the batch size of the\n",
    "    # model to calculate how many batches of data will be processed for each epoch.\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size,\n",
    "    # Specifies the number of epochs the model will go through. We set it to\n",
    "    # 50, so the training dataset will go through the model 50 times.\n",
    "    epochs = 50,\n",
    "    # Initializes the validation data the model will use to test its efficiency.\n",
    "    validation_data = test_generator,\n",
    "    # Calculates how many batches of data will be processed for each validation\n",
    "    # epoch. \n",
    "    validation_steps = test_generator.samples // test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d23a01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 1s 7ms/step - loss: 6.9536 - accuracy: 0.4819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.953614711761475, 0.48188909888267517]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the model on the test data.\n",
    "cnn.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9e580",
   "metadata": {},
   "source": [
    "Adding the parameter of 'zoom_range' in this third model didn't seem to help much. The model's accuracy was 48.19%, and the loss value was 6.95. Let's try adding another parameter to see if it gets better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb812d",
   "metadata": {},
   "source": [
    "###### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1e90d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Almost everything will be the same as the third model, except we will\n",
    "# add a parameter of 'horizontal_flip' to the 'train_datagenerator' variable.\n",
    "\n",
    "# Initializes the training image data generator.\n",
    "train_datagenerator = ImageDataGenerator(\n",
    "    rescale = 1./255, # Rescales the images by dividing the pixels by 255.\n",
    "    shear_range = 0.2, # Slants the image by up to 20%.\n",
    "    zoom_range = 0.2,  # Zooms into our away from the image by up to 20%.\n",
    "    horizontal_flip = True) # Flips random pictures horizontally.\n",
    "\n",
    "# Initializes the testing image data generator, and also rescales the images\n",
    "# by dividing the pixels by 255, just like the training data.\n",
    "test_datagenerator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Initializes the training generator using the previously defined \n",
    "# 'train_datagenerator' variable to create batches of data from the\n",
    "# orig_training directory.\n",
    "train_generator = train_datagenerator.flow_from_directory(\n",
    "    orig_training,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions.\n",
    "\n",
    "# Initializes the testing generator using the previously defined\n",
    "# 'test_datagenerator' variable to create batches of data from the\n",
    "# orig_testing directory.\n",
    "test_generator = test_datagenerator.flow_from_directory(\n",
    "    orig_testing,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5989a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the Sequential model.\n",
    "cnn = Sequential()\n",
    "\n",
    "# Adds a 2D convolutional layer with 32 filters, all 3x3 pixels, sets the \n",
    "# activation method as ReLU to make all negative values zero, and defines\n",
    "# the input shape as 48x48 pixels.\n",
    "cnn.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', \n",
    "                 input_shape = (48, 48, 1)))\n",
    "# Adds a max pooling layer with a size of 2x2, which helps mitigate overfitting.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 64 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 128 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattens the data into a 1D vector.\n",
    "cnn.add(Flatten())\n",
    "# Adds a fully connected layer with 128 neurons.\n",
    "cnn.add(Dense(128, activation = 'relu'))\n",
    "# Adds a dropout layer to prevent overfitting. 50% of the inputs are changed\n",
    "# to zero during this training.\n",
    "cnn.add(Dropout(0.5))\n",
    "# Adds an output layer with 7 neurons (1 for each class) and converts the output\n",
    "# into probabilities (softmax). \n",
    "cnn.add(Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c1260a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the model ready for use with the photos, using the same parameters\n",
    "# as the model before. \n",
    "cnn.compile(optimizer = Adam(), loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "431084fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.7714 - accuracy: 0.2739 - val_loss: 1.6325 - val_accuracy: 0.3602\n",
      "Epoch 2/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.6160 - accuracy: 0.3725 - val_loss: 1.4720 - val_accuracy: 0.4304\n",
      "Epoch 3/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.5140 - accuracy: 0.4189 - val_loss: 1.3858 - val_accuracy: 0.4679\n",
      "Epoch 4/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.4760 - accuracy: 0.4331 - val_loss: 1.3936 - val_accuracy: 0.4674\n",
      "Epoch 5/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4554 - accuracy: 0.4442 - val_loss: 1.4510 - val_accuracy: 0.4551\n",
      "Epoch 6/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4361 - accuracy: 0.4536 - val_loss: 1.4276 - val_accuracy: 0.4614\n",
      "Epoch 7/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4349 - accuracy: 0.4552 - val_loss: 1.3120 - val_accuracy: 0.4909\n",
      "Epoch 8/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4457 - accuracy: 0.4566 - val_loss: 1.4745 - val_accuracy: 0.4537\n",
      "Epoch 9/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4580 - accuracy: 0.4568 - val_loss: 1.4310 - val_accuracy: 0.4746\n",
      "Epoch 10/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4735 - accuracy: 0.4565 - val_loss: 1.4512 - val_accuracy: 0.4510\n",
      "Epoch 11/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4775 - accuracy: 0.4543 - val_loss: 1.4310 - val_accuracy: 0.4570\n",
      "Epoch 12/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.4921 - accuracy: 0.4543 - val_loss: 1.3060 - val_accuracy: 0.5141\n",
      "Epoch 13/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.5088 - accuracy: 0.4532 - val_loss: 1.4128 - val_accuracy: 0.4674\n",
      "Epoch 14/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.4900 - accuracy: 0.4556 - val_loss: 1.4674 - val_accuracy: 0.4887\n",
      "Epoch 15/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.5311 - accuracy: 0.4511 - val_loss: 1.6479 - val_accuracy: 0.4403\n",
      "Epoch 16/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.5546 - accuracy: 0.4491 - val_loss: 1.5167 - val_accuracy: 0.4720\n",
      "Epoch 17/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.5789 - accuracy: 0.4477 - val_loss: 1.3692 - val_accuracy: 0.4987\n",
      "Epoch 18/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 1.5739 - accuracy: 0.4525 - val_loss: 1.4509 - val_accuracy: 0.4732\n",
      "Epoch 19/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.6200 - accuracy: 0.4413 - val_loss: 1.4458 - val_accuracy: 0.4907\n",
      "Epoch 20/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.6216 - accuracy: 0.4436 - val_loss: 1.5236 - val_accuracy: 0.4996\n",
      "Epoch 21/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.6679 - accuracy: 0.4438 - val_loss: 1.7550 - val_accuracy: 0.4671\n",
      "Epoch 22/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.7320 - accuracy: 0.4363 - val_loss: 1.5108 - val_accuracy: 0.4801\n",
      "Epoch 23/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.7146 - accuracy: 0.4378 - val_loss: 1.7300 - val_accuracy: 0.4763\n",
      "Epoch 24/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 1.7775 - accuracy: 0.4361 - val_loss: 1.5862 - val_accuracy: 0.4835\n",
      "Epoch 25/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.8093 - accuracy: 0.4369 - val_loss: 1.5211 - val_accuracy: 0.4936\n",
      "Epoch 26/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.7989 - accuracy: 0.4363 - val_loss: 1.5437 - val_accuracy: 0.5011\n",
      "Epoch 27/50\n",
      "897/897 [==============================] - 11s 12ms/step - loss: 1.9395 - accuracy: 0.4304 - val_loss: 1.5924 - val_accuracy: 0.4833\n",
      "Epoch 28/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 1.9417 - accuracy: 0.4311 - val_loss: 1.3706 - val_accuracy: 0.5134\n",
      "Epoch 29/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 1.9778 - accuracy: 0.4305 - val_loss: 1.8439 - val_accuracy: 0.4743\n",
      "Epoch 30/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.0775 - accuracy: 0.4245 - val_loss: 1.7120 - val_accuracy: 0.4967\n",
      "Epoch 31/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.1032 - accuracy: 0.4295 - val_loss: 1.9506 - val_accuracy: 0.4637\n",
      "Epoch 32/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.1693 - accuracy: 0.4234 - val_loss: 2.1101 - val_accuracy: 0.4021\n",
      "Epoch 33/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 2.1596 - accuracy: 0.4231 - val_loss: 1.4485 - val_accuracy: 0.5092\n",
      "Epoch 34/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.4263 - accuracy: 0.4190 - val_loss: 2.3526 - val_accuracy: 0.4083\n",
      "Epoch 35/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.3735 - accuracy: 0.4197 - val_loss: 1.7967 - val_accuracy: 0.4859\n",
      "Epoch 36/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.6198 - accuracy: 0.4149 - val_loss: 2.1511 - val_accuracy: 0.4781\n",
      "Epoch 37/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 2.4496 - accuracy: 0.4197 - val_loss: 1.6739 - val_accuracy: 0.4877\n",
      "Epoch 38/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 2.5615 - accuracy: 0.4193 - val_loss: 2.1119 - val_accuracy: 0.4653\n",
      "Epoch 39/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 2.5652 - accuracy: 0.4212 - val_loss: 2.4479 - val_accuracy: 0.4810\n",
      "Epoch 40/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 2.9059 - accuracy: 0.4122 - val_loss: 2.2773 - val_accuracy: 0.4780\n",
      "Epoch 41/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 2.9810 - accuracy: 0.4092 - val_loss: 2.1013 - val_accuracy: 0.4862\n",
      "Epoch 42/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 3.2536 - accuracy: 0.4086 - val_loss: 2.4805 - val_accuracy: 0.4957\n",
      "Epoch 43/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 3.2600 - accuracy: 0.4093 - val_loss: 2.6526 - val_accuracy: 0.4612\n",
      "Epoch 44/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 3.3428 - accuracy: 0.4130 - val_loss: 1.9909 - val_accuracy: 0.5093\n",
      "Epoch 45/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 3.4163 - accuracy: 0.4083 - val_loss: 2.5115 - val_accuracy: 0.4771\n",
      "Epoch 46/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 3.8170 - accuracy: 0.4047 - val_loss: 3.0911 - val_accuracy: 0.4577\n",
      "Epoch 47/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 3.7955 - accuracy: 0.4090 - val_loss: 2.5056 - val_accuracy: 0.4770\n",
      "Epoch 48/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 4.1193 - accuracy: 0.4074 - val_loss: 3.3916 - val_accuracy: 0.4538\n",
      "Epoch 49/50\n",
      "897/897 [==============================] - 11s 13ms/step - loss: 4.2490 - accuracy: 0.4112 - val_loss: 3.3956 - val_accuracy: 0.4602\n",
      "Epoch 50/50\n",
      "897/897 [==============================] - 12s 13ms/step - loss: 4.1879 - accuracy: 0.4083 - val_loss: 2.5473 - val_accuracy: 0.4901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3475dda10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fits the model to the training and validation data, running through the epochs.\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    # Tells the model to divide the number of samples by the batch size of the\n",
    "    # model to calculate how many batches of data will be processed for each epoch.\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size,\n",
    "    # Specifies the number of epochs the model will go through. We set it to\n",
    "    # 50, so the training dataset will go through the model 50 times.\n",
    "    epochs = 50,\n",
    "    # Initializes the validation data the model will use to test its efficiency.\n",
    "    validation_data = test_generator,\n",
    "    # Calculates how many batches of data will be processed for each validation\n",
    "    # epoch. \n",
    "    validation_steps = test_generator.samples // test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea865e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 1s 6ms/step - loss: 2.5491 - accuracy: 0.4900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.549060106277466, 0.4899693429470062]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the model on the test data.\n",
    "cnn.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4257b",
   "metadata": {},
   "source": [
    "Adding the parameter of 'horizontal_flip' in this fourth model seemed to help a little. The model's accuracy was 49.00%, and the loss value was 2.55. Let's try a bigger batch size to see if it improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d788d",
   "metadata": {},
   "source": [
    "###### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd80a510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Almost everything will be the same as the fourth model, except we will\n",
    "# make the batch size bigger for the train and test generators.\n",
    "\n",
    "# Initializes the training image data generator.\n",
    "train_datagenerator = ImageDataGenerator(\n",
    "    rescale = 1./255, # Rescales the images by dividing the pixels by 255.\n",
    "    shear_range = 0.2, # Slants the image by up to 20%.\n",
    "    zoom_range = 0.2,  # Zooms into our away from the image by up to 20%.\n",
    "    horizontal_flip = True) # Flips random pictures horizontally.\n",
    "\n",
    "# Initializes the testing image data generator, and also rescales the images\n",
    "# by dividing the pixels by 255, just like the training data.\n",
    "test_datagenerator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Initializes the training generator using the previously defined \n",
    "# 'train_datagenerator' variable to create batches of data from the\n",
    "# orig_training directory.\n",
    "train_generator = train_datagenerator.flow_from_directory(\n",
    "    orig_training,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 64, # Defines the batch size (64 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions.\n",
    "\n",
    "# Initializes the testing generator using the previously defined\n",
    "# 'test_datagenerator' variable to create batches of data from the\n",
    "# orig_testing directory.\n",
    "test_generator = test_datagenerator.flow_from_directory(\n",
    "    orig_testing,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 64, # Defines the batch size (64 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9432942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the Sequential model.\n",
    "cnn = Sequential()\n",
    "\n",
    "# Adds a 2D convolutional layer with 32 filters, all 3x3 pixels, sets the \n",
    "# activation method as ReLU to make all negative values zero, and defines\n",
    "# the input shape as 48x48 pixels.\n",
    "cnn.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', \n",
    "                 input_shape = (48, 48, 1)))\n",
    "# Adds a max pooling layer with a size of 2x2, which helps mitigate overfitting.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 64 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 128 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattens the data into a 1D vector.\n",
    "cnn.add(Flatten())\n",
    "# Adds a fully connected layer with 128 neurons.\n",
    "cnn.add(Dense(128, activation = 'relu'))\n",
    "# Adds a dropout layer to prevent overfitting. 50% of the inputs are changed\n",
    "# to zero during this training.\n",
    "cnn.add(Dropout(0.5))\n",
    "# Adds an output layer with 7 neurons (1 for each class) and converts the output\n",
    "# into probabilities (softmax). \n",
    "cnn.add(Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c75ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the model ready for use with the photos, using the same parameters\n",
    "# as the model before. \n",
    "cnn.compile(optimizer = Adam(), loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56315e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 8s 16ms/step - loss: 1.7691 - accuracy: 0.2745 - val_loss: 1.6293 - val_accuracy: 0.3637\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.6231 - accuracy: 0.3722 - val_loss: 1.4935 - val_accuracy: 0.4308\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 7s 17ms/step - loss: 1.5355 - accuracy: 0.4138 - val_loss: 1.4212 - val_accuracy: 0.4561\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.4946 - accuracy: 0.4329 - val_loss: 1.3821 - val_accuracy: 0.4689\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.4690 - accuracy: 0.4416 - val_loss: 1.3458 - val_accuracy: 0.4927\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.4471 - accuracy: 0.4518 - val_loss: 1.3333 - val_accuracy: 0.4831\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.4536 - accuracy: 0.4514 - val_loss: 1.3322 - val_accuracy: 0.5024\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.4502 - accuracy: 0.4575 - val_loss: 1.2863 - val_accuracy: 0.5144\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.4560 - accuracy: 0.4585 - val_loss: 1.3222 - val_accuracy: 0.4941\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.4542 - accuracy: 0.4566 - val_loss: 1.3064 - val_accuracy: 0.5049\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.4581 - accuracy: 0.4581 - val_loss: 1.3735 - val_accuracy: 0.4827\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.4571 - accuracy: 0.4623 - val_loss: 1.3309 - val_accuracy: 0.4975\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.4656 - accuracy: 0.4612 - val_loss: 1.2948 - val_accuracy: 0.5080\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.4709 - accuracy: 0.4632 - val_loss: 1.3503 - val_accuracy: 0.4999\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.4814 - accuracy: 0.4628 - val_loss: 1.3812 - val_accuracy: 0.4834\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.4953 - accuracy: 0.4589 - val_loss: 1.3379 - val_accuracy: 0.5038\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.5285 - accuracy: 0.4562 - val_loss: 1.3060 - val_accuracy: 0.5098\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 7s 17ms/step - loss: 1.5233 - accuracy: 0.4560 - val_loss: 1.5580 - val_accuracy: 0.4647\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 7s 17ms/step - loss: 1.5498 - accuracy: 0.4577 - val_loss: 1.5876 - val_accuracy: 0.4336\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.5579 - accuracy: 0.4589 - val_loss: 1.5588 - val_accuracy: 0.4640\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.5799 - accuracy: 0.4552 - val_loss: 1.5064 - val_accuracy: 0.4965\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 7s 17ms/step - loss: 1.6187 - accuracy: 0.4481 - val_loss: 1.4348 - val_accuracy: 0.4987\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.6280 - accuracy: 0.4502 - val_loss: 1.3937 - val_accuracy: 0.5027\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.6355 - accuracy: 0.4543 - val_loss: 1.4678 - val_accuracy: 0.5098\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.6752 - accuracy: 0.4490 - val_loss: 1.5268 - val_accuracy: 0.5102\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.7139 - accuracy: 0.4450 - val_loss: 1.4668 - val_accuracy: 0.4958\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.7281 - accuracy: 0.4486 - val_loss: 1.4070 - val_accuracy: 0.5073\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.8025 - accuracy: 0.4407 - val_loss: 1.8455 - val_accuracy: 0.4348\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.8184 - accuracy: 0.4383 - val_loss: 1.4455 - val_accuracy: 0.5025\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.8729 - accuracy: 0.4367 - val_loss: 1.5219 - val_accuracy: 0.5027\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.8557 - accuracy: 0.4429 - val_loss: 1.7635 - val_accuracy: 0.4859\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.9217 - accuracy: 0.4413 - val_loss: 1.4704 - val_accuracy: 0.4922\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.9516 - accuracy: 0.4383 - val_loss: 1.7389 - val_accuracy: 0.4823\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 1.9764 - accuracy: 0.4378 - val_loss: 2.2566 - val_accuracy: 0.4761\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 2.0670 - accuracy: 0.4338 - val_loss: 1.7917 - val_accuracy: 0.4357\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 2.0040 - accuracy: 0.4325 - val_loss: 1.9855 - val_accuracy: 0.4814\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 2.0599 - accuracy: 0.4354 - val_loss: 1.7084 - val_accuracy: 0.4681\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 2.2751 - accuracy: 0.4286 - val_loss: 1.9067 - val_accuracy: 0.5105\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 7s 17ms/step - loss: 2.1356 - accuracy: 0.4395 - val_loss: 1.8723 - val_accuracy: 0.4746\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 2.2103 - accuracy: 0.4311 - val_loss: 1.6725 - val_accuracy: 0.5247\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 2.3662 - accuracy: 0.4254 - val_loss: 2.3303 - val_accuracy: 0.4492\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 2.3311 - accuracy: 0.4284 - val_loss: 1.7795 - val_accuracy: 0.4891\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 2.3957 - accuracy: 0.4257 - val_loss: 1.8836 - val_accuracy: 0.4812\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 2.5125 - accuracy: 0.4254 - val_loss: 1.8395 - val_accuracy: 0.5089\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 2.4449 - accuracy: 0.4234 - val_loss: 2.1957 - val_accuracy: 0.4704\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 7s 17ms/step - loss: 2.8049 - accuracy: 0.4199 - val_loss: 2.2171 - val_accuracy: 0.4729\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 2.7498 - accuracy: 0.4235 - val_loss: 2.8566 - val_accuracy: 0.4633\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 2.5764 - accuracy: 0.4278 - val_loss: 2.7988 - val_accuracy: 0.3873\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 2.7975 - accuracy: 0.4237 - val_loss: 1.8683 - val_accuracy: 0.4954\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 2.9373 - accuracy: 0.4231 - val_loss: 2.8203 - val_accuracy: 0.4668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x320e22d90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fits the model to the training and validation data, running through the epochs.\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    # Tells the model to divide the number of samples by the batch size of the\n",
    "    # model to calculate how many batches of data will be processed for each epoch.\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size,\n",
    "    # Specifies the number of epochs the model will go through. We set it to\n",
    "    # 50, so the training dataset will go through the model 50 times.\n",
    "    epochs = 50,\n",
    "    # Initializes the validation data the model will use to test its efficiency.\n",
    "    validation_data = test_generator,\n",
    "    # Calculates how many batches of data will be processed for each validation\n",
    "    # epoch. \n",
    "    validation_steps = test_generator.samples // test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2de734ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 1s 7ms/step - loss: 2.8223 - accuracy: 0.4667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8223371505737305, 0.4667038023471832]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the model on the test data.\n",
    "cnn.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44edd3b8",
   "metadata": {},
   "source": [
    "Changing the batch size in this fifth model seemed to not help. The model's accuracy was 46.67%, and the loss value was 2.82. Let's try adding a batch normalization function in the model set-up to see if it improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cfdd8c",
   "metadata": {},
   "source": [
    "###### Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a193522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Everything here will be the same as the fifth model.\n",
    "\n",
    "# Initializes the training image data generator.\n",
    "train_datagenerator = ImageDataGenerator(\n",
    "    rescale = 1./255, # Rescales the images by dividing the pixels by 255.\n",
    "    shear_range = 0.2, # Slants the image by up to 20%.\n",
    "    zoom_range = 0.2,  # Zooms into our away from the image by up to 20%.\n",
    "    horizontal_flip = True) # Flips random pictures horizontally.\n",
    "\n",
    "# Initializes the testing image data generator, and also rescales the images\n",
    "# by dividing the pixels by 255, just like the training data.\n",
    "test_datagenerator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Initializes the training generator using the previously defined \n",
    "# 'train_datagenerator' variable to create batches of data from the\n",
    "# orig_training directory.\n",
    "train_generator = train_datagenerator.flow_from_directory(\n",
    "    orig_training,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 64, # Defines the batch size (64 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions.\n",
    "\n",
    "# Initializes the testing generator using the previously defined\n",
    "# 'test_datagenerator' variable to create batches of data from the\n",
    "# orig_testing directory.\n",
    "test_generator = test_datagenerator.flow_from_directory(\n",
    "    orig_testing,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 64, # Defines the batch size (64 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1846e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will add Batch Normalization here to see how it impacts the model.\n",
    "\n",
    "# Initializes the Sequential model.\n",
    "cnn = Sequential()\n",
    "\n",
    "# Adds a 2D convolutional layer with 32 filters, all 3x3 pixels, sets the \n",
    "# activation method as ReLU to make all negative values zero, and defines\n",
    "# the input shape as 48x48 pixels.\n",
    "cnn.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', \n",
    "                 input_shape = (48, 48, 1)))\n",
    "cnn.add(BatchNormalization()) # Adds Batch Normalization to the model.\n",
    "# Adds a max pooling layer with a size of 2x2, which helps mitigate overfitting.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 64 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(BatchNormalization()) # Adds Batch Normalization to the model.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 128 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(BatchNormalization()) # Adds Batch Normalization to the model.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattens the data into a 1D vector.\n",
    "cnn.add(Flatten())\n",
    "# Adds a fully connected layer with 128 neurons.\n",
    "cnn.add(Dense(128, activation = 'relu'))\n",
    "# Adds a dropout layer to prevent overfitting. 50% of the inputs are changed\n",
    "# to zero during this training.\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(BatchNormalization()) # Adds Batch Normalization to the model.\n",
    "# Adds an output layer with 7 neurons (1 for each class) and converts the output\n",
    "# into probabilities (softmax). \n",
    "cnn.add(Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b85b6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the model ready for use with the photos, using the same parameters\n",
    "# as before. \n",
    "cnn.compile(optimizer = Adam(), loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3d5d38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 1.7440 - accuracy: 0.3486 - val_loss: 1.6753 - val_accuracy: 0.3612\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 1.4300 - accuracy: 0.4553 - val_loss: 1.3882 - val_accuracy: 0.4616\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 1.3200 - accuracy: 0.5002 - val_loss: 1.3542 - val_accuracy: 0.4724\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 1.2673 - accuracy: 0.5204 - val_loss: 1.2433 - val_accuracy: 0.5331\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 1.2255 - accuracy: 0.5387 - val_loss: 1.2534 - val_accuracy: 0.5215\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 1.1977 - accuracy: 0.5511 - val_loss: 1.2081 - val_accuracy: 0.5407\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 1.1761 - accuracy: 0.5585 - val_loss: 1.2309 - val_accuracy: 0.5402\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 1.1505 - accuracy: 0.5650 - val_loss: 1.2411 - val_accuracy: 0.5294\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 1.1365 - accuracy: 0.5720 - val_loss: 1.1162 - val_accuracy: 0.5837\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 1.1190 - accuracy: 0.5806 - val_loss: 1.1160 - val_accuracy: 0.5806\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 8s 19ms/step - loss: 1.1113 - accuracy: 0.5817 - val_loss: 1.2050 - val_accuracy: 0.5488\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 1.0968 - accuracy: 0.5878 - val_loss: 1.2036 - val_accuracy: 0.5413\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 1.0827 - accuracy: 0.5956 - val_loss: 1.3113 - val_accuracy: 0.5187\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 1.0689 - accuracy: 0.6008 - val_loss: 1.1975 - val_accuracy: 0.5573\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 1.0575 - accuracy: 0.6096 - val_loss: 1.0737 - val_accuracy: 0.6003\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 1.0462 - accuracy: 0.6104 - val_loss: 1.0763 - val_accuracy: 0.5965\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 1.0420 - accuracy: 0.6128 - val_loss: 1.0940 - val_accuracy: 0.5886\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 9s 21ms/step - loss: 1.0314 - accuracy: 0.6165 - val_loss: 1.2151 - val_accuracy: 0.5525\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 1.0310 - accuracy: 0.6187 - val_loss: 1.0939 - val_accuracy: 0.5965\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 8s 19ms/step - loss: 1.0259 - accuracy: 0.6210 - val_loss: 1.0777 - val_accuracy: 0.6007\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 1.0108 - accuracy: 0.6278 - val_loss: 1.1606 - val_accuracy: 0.5685\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 0.9987 - accuracy: 0.6302 - val_loss: 1.0991 - val_accuracy: 0.5947\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 0.9937 - accuracy: 0.6339 - val_loss: 1.1917 - val_accuracy: 0.5590\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 0.9905 - accuracy: 0.6337 - val_loss: 1.0540 - val_accuracy: 0.6025\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 0.9832 - accuracy: 0.6364 - val_loss: 1.0634 - val_accuracy: 0.5997\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 0.9814 - accuracy: 0.6355 - val_loss: 1.0663 - val_accuracy: 0.6028\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 0.9713 - accuracy: 0.6454 - val_loss: 1.1004 - val_accuracy: 0.5858\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 0.9649 - accuracy: 0.6451 - val_loss: 1.0668 - val_accuracy: 0.5984\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 0.9648 - accuracy: 0.6431 - val_loss: 1.0783 - val_accuracy: 0.6000\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 0.9581 - accuracy: 0.6468 - val_loss: 1.2020 - val_accuracy: 0.5600\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 0.9549 - accuracy: 0.6470 - val_loss: 1.0615 - val_accuracy: 0.6084\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 0.9533 - accuracy: 0.6487 - val_loss: 1.1668 - val_accuracy: 0.5681\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 0.9417 - accuracy: 0.6526 - val_loss: 1.0936 - val_accuracy: 0.5972\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 0.9415 - accuracy: 0.6550 - val_loss: 1.1014 - val_accuracy: 0.6046\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 0.9366 - accuracy: 0.6538 - val_loss: 1.0697 - val_accuracy: 0.5992\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 0.9247 - accuracy: 0.6571 - val_loss: 1.0638 - val_accuracy: 0.5991\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 0.9246 - accuracy: 0.6573 - val_loss: 1.1119 - val_accuracy: 0.5893\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 0.9212 - accuracy: 0.6603 - val_loss: 1.0928 - val_accuracy: 0.5974\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 0.9186 - accuracy: 0.6598 - val_loss: 1.0906 - val_accuracy: 0.5996\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 0.9182 - accuracy: 0.6633 - val_loss: 1.0552 - val_accuracy: 0.6076\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 0.9215 - accuracy: 0.6641 - val_loss: 1.1630 - val_accuracy: 0.5780\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 0.9096 - accuracy: 0.6627 - val_loss: 1.0795 - val_accuracy: 0.6032\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 0.9091 - accuracy: 0.6657 - val_loss: 1.0862 - val_accuracy: 0.6083\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 0.9067 - accuracy: 0.6674 - val_loss: 1.0832 - val_accuracy: 0.6009\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 0.9051 - accuracy: 0.6701 - val_loss: 1.1055 - val_accuracy: 0.5917\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 0.8893 - accuracy: 0.6747 - val_loss: 1.0681 - val_accuracy: 0.6052\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 9s 20ms/step - loss: 0.8967 - accuracy: 0.6717 - val_loss: 1.0886 - val_accuracy: 0.6053\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 0.8909 - accuracy: 0.6765 - val_loss: 1.0950 - val_accuracy: 0.6006\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 8s 19ms/step - loss: 0.8883 - accuracy: 0.6738 - val_loss: 1.0948 - val_accuracy: 0.5957\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 9s 19ms/step - loss: 0.8826 - accuracy: 0.6765 - val_loss: 1.1718 - val_accuracy: 0.5791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x359542d90>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fits the model to the training and validation data, running through the epochs.\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    # Tells the model to divide the number of samples by the batch size of the\n",
    "    # model to calculate how many batches of data will be processed for each epoch.\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size,\n",
    "    # Specifies the number of epochs the model will go through. We set it to\n",
    "    # 50, so the training dataset will go through the model 50 times.\n",
    "    epochs = 50,\n",
    "    # Initializes the validation data the model will use to test its efficiency.\n",
    "    validation_data = test_generator,\n",
    "    # Calculates how many batches of data will be processed for each validation\n",
    "    # epoch. \n",
    "    validation_steps = test_generator.samples // test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "723c558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 1s 8ms/step - loss: 1.1719 - accuracy: 0.5791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1719416379928589, 0.5791306495666504]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the model on the test data.\n",
    "cnn.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5dc867",
   "metadata": {},
   "source": [
    "Adding a batch normalization function in this sixth model seemed to help a lot. The model's accuracy was 57.91%, and the loss value was 1.17. Let's try adding a batch normalization function in the model set-up to see if it improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a61887",
   "metadata": {},
   "source": [
    "##### Running Models on the Cleaned Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59614464",
   "metadata": {},
   "source": [
    "###### Model 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d2d7b8",
   "metadata": {},
   "source": [
    "The dataset originally included photos that were not of facial expressions (e.g., some where just black squares, some were only hands, and some where just a person's torso). In order to make sure the data only included facial expressions, we manually went through all of the photos and deleted any non-facial expression photos. \n",
    "\n",
    "Now, let's try running the same models on the cleaned data to see if they work better with a cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2c2abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets variables to the name of the folder the clean data is saved in.\n",
    "training = 'train'\n",
    "testing = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b45cfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28645 images belonging to 7 classes.\n",
      "Found 7164 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Initializes the training image data generator and\n",
    "# rescales the images by dividing the pixels by 255.\n",
    "train_datagenerator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Initializes the testing image data generator, and also rescales the images\n",
    "# by dividing the pixels by 255, just like the training data.\n",
    "test_datagenerator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Initializes the training generator using the previously defined \n",
    "# 'train_datagenerator' variable to create batches of data from the\n",
    "# training directory.\n",
    "train_generator = train_datagenerator.flow_from_directory(\n",
    "    training,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions.\n",
    "\n",
    "# Initializes the testing generator using the previously defined\n",
    "# 'test_datagenerator' variable to create batches of data from the\n",
    "# testing directory.\n",
    "test_generator = test_datagenerator.flow_from_directory(\n",
    "    testing,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85924293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CNN model of 'Sequential' allows for us to add layers to the model\n",
    "# one at a time. This initializes the model.\n",
    "cnn = Sequential()\n",
    "\n",
    "# Adds a 2D convolutional layer with 32 filters, all 3x3 pixels, sets the \n",
    "# activation method as ReLU to make all negative values zero, and defines\n",
    "# the input shape as 48x48 pixels.\n",
    "cnn.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', \n",
    "                 input_shape = (48, 48, 1)))\n",
    "# Adds a max pooling layer with a size of 2x2, which helps mitigate overfitting.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 64 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 128 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattens the data into a 1D vector.\n",
    "cnn.add(Flatten())\n",
    "# Adds a fully connected layer with 128 neurons.\n",
    "cnn.add(Dense(128, activation = 'relu'))\n",
    "# Adds a dropout layer to prevent overfitting. 50% of the inputs are changed\n",
    "# to zero during this training.\n",
    "cnn.add(Dropout(0.5))\n",
    "# Adds an output layer with 7 neurons (1 for each class) and converts the output\n",
    "# into probabilities (softmax). \n",
    "cnn.add(Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44d94732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the model ready for use with the photos, using the same parameters\n",
    "# as before. \n",
    "cnn.compile(optimizer = Adam(), loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83aacec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.7657 - accuracy: 0.2823 - val_loss: 1.6276 - val_accuracy: 0.3793\n",
      "Epoch 2/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 1.5859 - accuracy: 0.3858 - val_loss: 1.4534 - val_accuracy: 0.4460\n",
      "Epoch 3/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 1.4825 - accuracy: 0.4304 - val_loss: 1.4017 - val_accuracy: 0.4619\n",
      "Epoch 4/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4286 - accuracy: 0.4540 - val_loss: 1.3519 - val_accuracy: 0.4851\n",
      "Epoch 5/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 1.4073 - accuracy: 0.4660 - val_loss: 1.3627 - val_accuracy: 0.4814\n",
      "Epoch 6/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3914 - accuracy: 0.4730 - val_loss: 1.3257 - val_accuracy: 0.4929\n",
      "Epoch 7/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3953 - accuracy: 0.4734 - val_loss: 1.3507 - val_accuracy: 0.4903\n",
      "Epoch 8/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3957 - accuracy: 0.4772 - val_loss: 1.4607 - val_accuracy: 0.4508\n",
      "Epoch 9/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4065 - accuracy: 0.4767 - val_loss: 1.3402 - val_accuracy: 0.4931\n",
      "Epoch 10/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4150 - accuracy: 0.4787 - val_loss: 1.4258 - val_accuracy: 0.4612\n",
      "Epoch 11/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4177 - accuracy: 0.4848 - val_loss: 1.3122 - val_accuracy: 0.5059\n",
      "Epoch 12/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4601 - accuracy: 0.4785 - val_loss: 1.4402 - val_accuracy: 0.4741\n",
      "Epoch 13/50\n",
      "895/895 [==============================] - 13s 14ms/step - loss: 1.4734 - accuracy: 0.4778 - val_loss: 1.3254 - val_accuracy: 0.5004\n",
      "Epoch 14/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4833 - accuracy: 0.4821 - val_loss: 1.5169 - val_accuracy: 0.4613\n",
      "Epoch 15/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.5293 - accuracy: 0.4769 - val_loss: 1.6711 - val_accuracy: 0.4404\n",
      "Epoch 16/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.5422 - accuracy: 0.4779 - val_loss: 1.4266 - val_accuracy: 0.4968\n",
      "Epoch 17/50\n",
      "895/895 [==============================] - 190s 213ms/step - loss: 1.5889 - accuracy: 0.4743 - val_loss: 1.4377 - val_accuracy: 0.4864\n",
      "Epoch 18/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 1.6324 - accuracy: 0.4708 - val_loss: 1.6423 - val_accuracy: 0.4623\n",
      "Epoch 19/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.6933 - accuracy: 0.4692 - val_loss: 1.5543 - val_accuracy: 0.4521\n",
      "Epoch 20/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.7188 - accuracy: 0.4683 - val_loss: 1.4678 - val_accuracy: 0.4776\n",
      "Epoch 21/50\n",
      "895/895 [==============================] - 912s 1s/step - loss: 1.8053 - accuracy: 0.4687 - val_loss: 1.3842 - val_accuracy: 0.4793\n",
      "Epoch 22/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 1.8685 - accuracy: 0.4650 - val_loss: 1.6590 - val_accuracy: 0.4891\n",
      "Epoch 23/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 1.9190 - accuracy: 0.4660 - val_loss: 1.8840 - val_accuracy: 0.4368\n",
      "Epoch 24/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 2.0155 - accuracy: 0.4634 - val_loss: 1.6036 - val_accuracy: 0.4999\n",
      "Epoch 25/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 2.1007 - accuracy: 0.4600 - val_loss: 2.3314 - val_accuracy: 0.3649\n",
      "Epoch 26/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 2.2227 - accuracy: 0.4552 - val_loss: 1.5033 - val_accuracy: 0.5254\n",
      "Epoch 27/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.2625 - accuracy: 0.4592 - val_loss: 1.9272 - val_accuracy: 0.4849\n",
      "Epoch 28/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.3498 - accuracy: 0.4546 - val_loss: 2.4459 - val_accuracy: 0.4260\n",
      "Epoch 29/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 2.6020 - accuracy: 0.4480 - val_loss: 1.9166 - val_accuracy: 0.4961\n",
      "Epoch 30/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 2.5703 - accuracy: 0.4576 - val_loss: 2.5015 - val_accuracy: 0.3935\n",
      "Epoch 31/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 2.8264 - accuracy: 0.4478 - val_loss: 1.8902 - val_accuracy: 0.4805\n",
      "Epoch 32/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 3.0739 - accuracy: 0.4519 - val_loss: 2.2077 - val_accuracy: 0.4858\n",
      "Epoch 33/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 3.1090 - accuracy: 0.4550 - val_loss: 2.6834 - val_accuracy: 0.4595\n",
      "Epoch 34/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 3.3655 - accuracy: 0.4477 - val_loss: 2.4644 - val_accuracy: 0.4617\n",
      "Epoch 35/50\n",
      "895/895 [==============================] - 356s 399ms/step - loss: 3.5437 - accuracy: 0.4481 - val_loss: 3.3234 - val_accuracy: 0.4566\n",
      "Epoch 36/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 4.0017 - accuracy: 0.4473 - val_loss: 3.1406 - val_accuracy: 0.4617\n",
      "Epoch 37/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 3.8233 - accuracy: 0.4515 - val_loss: 3.3834 - val_accuracy: 0.4126\n",
      "Epoch 38/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 4.1240 - accuracy: 0.4470 - val_loss: 2.9532 - val_accuracy: 0.4805\n",
      "Epoch 39/50\n",
      "895/895 [==============================] - 916s 1s/step - loss: 4.2465 - accuracy: 0.4486 - val_loss: 5.3899 - val_accuracy: 0.4274\n",
      "Epoch 40/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 4.7543 - accuracy: 0.4516 - val_loss: 4.0633 - val_accuracy: 0.4190\n",
      "Epoch 41/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 5.0160 - accuracy: 0.4474 - val_loss: 4.0556 - val_accuracy: 0.4728\n",
      "Epoch 42/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 5.5008 - accuracy: 0.4478 - val_loss: 4.6774 - val_accuracy: 0.4990\n",
      "Epoch 43/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 5.7687 - accuracy: 0.4462 - val_loss: 4.7160 - val_accuracy: 0.4951\n",
      "Epoch 44/50\n",
      "895/895 [==============================] - 216s 241ms/step - loss: 5.9318 - accuracy: 0.4506 - val_loss: 6.9757 - val_accuracy: 0.3751\n",
      "Epoch 45/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 6.4939 - accuracy: 0.4482 - val_loss: 5.6326 - val_accuracy: 0.4575\n",
      "Epoch 46/50\n",
      "895/895 [==============================] - 11s 12ms/step - loss: 6.7798 - accuracy: 0.4462 - val_loss: 5.5411 - val_accuracy: 0.4847\n",
      "Epoch 47/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 7.1801 - accuracy: 0.4508 - val_loss: 5.0195 - val_accuracy: 0.4874\n",
      "Epoch 48/50\n",
      "895/895 [==============================] - 73s 81ms/step - loss: 7.4502 - accuracy: 0.4471 - val_loss: 6.3388 - val_accuracy: 0.4779\n",
      "Epoch 49/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 8.3002 - accuracy: 0.4548 - val_loss: 13.3724 - val_accuracy: 0.4074\n",
      "Epoch 50/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 8.6484 - accuracy: 0.4442 - val_loss: 8.4846 - val_accuracy: 0.4727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x36f8f01d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fits the model to the training and validation data, running through the epochs.\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    # Tells the model to divide the number of samples by the batch size of the\n",
    "    # model to calculate how many batches of data will be processed for each epoch.\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size,\n",
    "    # Specifies the number of epochs the model will go through. We set it to\n",
    "    # 50, so the training dataset will go through the model 50 times.\n",
    "    epochs = 50,\n",
    "    # Initializes the validation data the model will use to test its efficiency.\n",
    "    validation_data = test_generator,\n",
    "    # Calculates how many batches of data will be processed for each validation\n",
    "    # epoch. \n",
    "    validation_steps = test_generator.samples // test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23093f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/224 [==============================] - 2s 7ms/step - loss: 8.4651 - accuracy: 0.4731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.465080261230469, 0.473059743642807]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the model on the test data.\n",
    "cnn.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b523e4",
   "metadata": {},
   "source": [
    "Using clean data in this seventh model seemed to help when compared to the first model using the original data. The model's accuracy was 47.31%, and the loss value was 8.47. Now, let's try adding the same parameters as in the original models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33d503",
   "metadata": {},
   "source": [
    "###### Model 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a77e91a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28645 images belonging to 7 classes.\n",
      "Found 7164 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Almost everything will be the same as the seventh model, except we will\n",
    "# add a parameter of 'shear_range' to the 'train_datagenerator' variable.\n",
    "\n",
    "# Initializes the training image data generator.\n",
    "train_datagenerator = ImageDataGenerator(\n",
    "    rescale = 1./255, # Rescales the images by dividing the pixels by 255.\n",
    "    shear_range = 0.2) # Applies a shear transformation to the images (slanting\n",
    "                       # the image by up to 20%).\n",
    "\n",
    "# Initializes the testing image data generator, and also rescales the images\n",
    "# by dividing the pixels by 255, just like the training data.\n",
    "test_datagenerator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Initializes the training generator using the previously defined \n",
    "# 'train_datagenerator' variable to create batches of data from the\n",
    "# training directory.\n",
    "train_generator = train_datagenerator.flow_from_directory(\n",
    "    training,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions.\n",
    "\n",
    "# Initializes the testing generator using the previously defined\n",
    "# 'test_datagenerator' variable to create batches of data from the\n",
    "# testing directory.\n",
    "test_generator = test_datagenerator.flow_from_directory(\n",
    "    testing,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5623de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the Sequential model.\n",
    "cnn = Sequential()\n",
    "\n",
    "# Adds a 2D convolutional layer with 32 filters, all 3x3 pixels, sets the \n",
    "# activation method as ReLU to make all negative values zero, and defines\n",
    "# the input shape as 48x48 pixels.\n",
    "cnn.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', \n",
    "                 input_shape = (48, 48, 1)))\n",
    "# Adds a max pooling layer with a size of 2x2, which helps mitigate overfitting.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 64 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 128 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattens the data into a 1D vector.\n",
    "cnn.add(Flatten())\n",
    "# Adds a fully connected layer with 128 neurons.\n",
    "cnn.add(Dense(128, activation = 'relu'))\n",
    "# Adds a dropout layer to prevent overfitting. 50% of the inputs are changed\n",
    "# to zero during this training.\n",
    "cnn.add(Dropout(0.5))\n",
    "# Adds an output layer with 7 neurons (1 for each class) and converts the output\n",
    "# into probabilities (softmax). \n",
    "cnn.add(Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b8464c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the model ready for use with the photos, using the same parameters\n",
    "# as before. \n",
    "cnn.compile(optimizer = Adam(), loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b27f4e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "895/895 [==============================] - 13s 14ms/step - loss: 1.7432 - accuracy: 0.2943 - val_loss: 1.5916 - val_accuracy: 0.3681\n",
      "Epoch 2/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.5425 - accuracy: 0.4050 - val_loss: 1.4242 - val_accuracy: 0.4367\n",
      "Epoch 3/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4644 - accuracy: 0.4434 - val_loss: 1.3606 - val_accuracy: 0.4769\n",
      "Epoch 4/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4386 - accuracy: 0.4605 - val_loss: 1.3977 - val_accuracy: 0.4710\n",
      "Epoch 5/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4364 - accuracy: 0.4634 - val_loss: 1.3668 - val_accuracy: 0.4716\n",
      "Epoch 6/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4444 - accuracy: 0.4684 - val_loss: 1.4180 - val_accuracy: 0.4563\n",
      "Epoch 7/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4913 - accuracy: 0.4640 - val_loss: 1.4310 - val_accuracy: 0.4610\n",
      "Epoch 8/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.5252 - accuracy: 0.4606 - val_loss: 1.4531 - val_accuracy: 0.4521\n",
      "Epoch 9/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.5728 - accuracy: 0.4587 - val_loss: 1.6710 - val_accuracy: 0.4438\n",
      "Epoch 10/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.6262 - accuracy: 0.4596 - val_loss: 1.6589 - val_accuracy: 0.4427\n",
      "Epoch 11/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.6895 - accuracy: 0.4572 - val_loss: 1.5211 - val_accuracy: 0.4647\n",
      "Epoch 12/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.7671 - accuracy: 0.4591 - val_loss: 1.8133 - val_accuracy: 0.4360\n",
      "Epoch 13/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.8740 - accuracy: 0.4511 - val_loss: 1.8958 - val_accuracy: 0.4305\n",
      "Epoch 14/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.0013 - accuracy: 0.4497 - val_loss: 2.0607 - val_accuracy: 0.4259\n",
      "Epoch 15/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.1403 - accuracy: 0.4457 - val_loss: 2.2545 - val_accuracy: 0.4340\n",
      "Epoch 16/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.2665 - accuracy: 0.4444 - val_loss: 1.7593 - val_accuracy: 0.4561\n",
      "Epoch 17/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.4210 - accuracy: 0.4426 - val_loss: 2.5061 - val_accuracy: 0.4016\n",
      "Epoch 18/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.6439 - accuracy: 0.4385 - val_loss: 1.9112 - val_accuracy: 0.4986\n",
      "Epoch 19/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.8945 - accuracy: 0.4416 - val_loss: 1.9895 - val_accuracy: 0.4825\n",
      "Epoch 20/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 3.2843 - accuracy: 0.4334 - val_loss: 2.8981 - val_accuracy: 0.4672\n",
      "Epoch 21/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 3.4683 - accuracy: 0.4387 - val_loss: 2.7138 - val_accuracy: 0.4442\n",
      "Epoch 22/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 3.6464 - accuracy: 0.4377 - val_loss: 2.2813 - val_accuracy: 0.4945\n",
      "Epoch 23/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 4.3647 - accuracy: 0.4315 - val_loss: 2.9016 - val_accuracy: 0.4821\n",
      "Epoch 24/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 4.7022 - accuracy: 0.4340 - val_loss: 3.6075 - val_accuracy: 0.4123\n",
      "Epoch 25/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 5.4489 - accuracy: 0.4338 - val_loss: 3.9115 - val_accuracy: 0.4697\n",
      "Epoch 26/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 5.8372 - accuracy: 0.4331 - val_loss: 6.6790 - val_accuracy: 0.4098\n",
      "Epoch 27/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 6.6306 - accuracy: 0.4349 - val_loss: 4.4227 - val_accuracy: 0.4536\n",
      "Epoch 28/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 6.8396 - accuracy: 0.4400 - val_loss: 5.3806 - val_accuracy: 0.4409\n",
      "Epoch 29/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 7.8823 - accuracy: 0.4333 - val_loss: 12.9623 - val_accuracy: 0.3758\n",
      "Epoch 30/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 8.6030 - accuracy: 0.4378 - val_loss: 8.3196 - val_accuracy: 0.4148\n",
      "Epoch 31/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 10.1982 - accuracy: 0.4316 - val_loss: 11.4127 - val_accuracy: 0.4483\n",
      "Epoch 32/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 10.4680 - accuracy: 0.4399 - val_loss: 9.6752 - val_accuracy: 0.4250\n",
      "Epoch 33/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 10.9561 - accuracy: 0.4347 - val_loss: 10.2310 - val_accuracy: 0.4494\n",
      "Epoch 34/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 12.7926 - accuracy: 0.4335 - val_loss: 8.2038 - val_accuracy: 0.4697\n",
      "Epoch 35/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 12.9756 - accuracy: 0.4435 - val_loss: 14.2869 - val_accuracy: 0.4727\n",
      "Epoch 36/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 14.9579 - accuracy: 0.4370 - val_loss: 9.6856 - val_accuracy: 0.4959\n",
      "Epoch 37/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 16.0152 - accuracy: 0.4360 - val_loss: 10.4318 - val_accuracy: 0.4753\n",
      "Epoch 38/50\n",
      "895/895 [==============================] - 15s 16ms/step - loss: 17.2860 - accuracy: 0.4398 - val_loss: 19.3486 - val_accuracy: 0.3971\n",
      "Epoch 39/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 20.4805 - accuracy: 0.4367 - val_loss: 11.1186 - val_accuracy: 0.5098\n",
      "Epoch 40/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 22.7810 - accuracy: 0.4376 - val_loss: 24.0496 - val_accuracy: 0.4040\n",
      "Epoch 41/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 23.9530 - accuracy: 0.4398 - val_loss: 20.1296 - val_accuracy: 0.4308\n",
      "Epoch 42/50\n",
      "895/895 [==============================] - 13s 14ms/step - loss: 25.8910 - accuracy: 0.4410 - val_loss: 20.7397 - val_accuracy: 0.4444\n",
      "Epoch 43/50\n",
      "895/895 [==============================] - 13s 14ms/step - loss: 28.7427 - accuracy: 0.4413 - val_loss: 28.2511 - val_accuracy: 0.4009\n",
      "Epoch 44/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 31.5310 - accuracy: 0.4392 - val_loss: 23.0762 - val_accuracy: 0.4711\n",
      "Epoch 45/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 32.3612 - accuracy: 0.4423 - val_loss: 29.5802 - val_accuracy: 0.4659\n",
      "Epoch 46/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 34.1628 - accuracy: 0.4457 - val_loss: 51.3484 - val_accuracy: 0.3608\n",
      "Epoch 47/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 38.0999 - accuracy: 0.4370 - val_loss: 36.8343 - val_accuracy: 0.4351\n",
      "Epoch 48/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 41.4095 - accuracy: 0.4424 - val_loss: 32.4016 - val_accuracy: 0.4522\n",
      "Epoch 49/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 43.7983 - accuracy: 0.4416 - val_loss: 40.7982 - val_accuracy: 0.4877\n",
      "Epoch 50/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 46.6046 - accuracy: 0.4439 - val_loss: 44.5217 - val_accuracy: 0.4298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x377e22d90>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fits the model to the training and validation data, running through the epochs.\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    # Tells the model to divide the number of samples by the batch size of the\n",
    "    # model to calculate how many batches of data will be processed for each epoch.\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size,\n",
    "    # Specifies the number of epochs the model will go through. We set it to\n",
    "    # 50, so the training dataset will go through the model 50 times.\n",
    "    epochs = 50,\n",
    "    # Initializes the validation data the model will use to test its efficiency.\n",
    "    validation_data = test_generator,\n",
    "    # Calculates how many batches of data will be processed for each validation\n",
    "    # epoch. \n",
    "    validation_steps = test_generator.samples // test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b987df3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/224 [==============================] - 2s 7ms/step - loss: 44.4317 - accuracy: 0.4308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[44.43173599243164, 0.43076494336128235]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the model on the test data.\n",
    "cnn.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10c74f",
   "metadata": {},
   "source": [
    "Adding the parameter of 'shear_range' in this second model seemed to not help. The model's accuracy was 43.08%, and the loss value was 44.43. Let's try adding another parameter to see if it gets better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69496a98",
   "metadata": {},
   "source": [
    "###### Model 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b900b8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28645 images belonging to 7 classes.\n",
      "Found 7164 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Almost everything will be the same as the previous model, except we will\n",
    "# add a parameter of 'zoom_range' to the 'train_datagenerator' variable.\n",
    "\n",
    "# Initializes the training image data generator.\n",
    "train_datagenerator = ImageDataGenerator(\n",
    "    rescale = 1./255, # Rescales the images by dividing the pixels by 255.\n",
    "    shear_range = 0.2, # Slants the image by up to 20%.\n",
    "    zoom_range = 0.2)  # Zooms into our away from the image by up to 20%.\n",
    "\n",
    "# Initializes the testing image data generator, and also rescales the images\n",
    "# by dividing the pixels by 255, just like the training data.\n",
    "test_datagenerator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Initializes the training generator using the previously defined \n",
    "# 'train_datagenerator' variable to create batches of data from the\n",
    "# training directory.\n",
    "train_generator = train_datagenerator.flow_from_directory(\n",
    "    training,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions.\n",
    "\n",
    "# Initializes the testing generator using the previously defined\n",
    "# 'test_datagenerator' variable to create batches of data from the\n",
    "# testing directory.\n",
    "test_generator = test_datagenerator.flow_from_directory(\n",
    "    testing,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60d92df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the Sequential model.\n",
    "cnn = Sequential()\n",
    "\n",
    "# Adds a 2D convolutional layer with 32 filters, all 3x3 pixels, sets the \n",
    "# activation method as ReLU to make all negative values zero, and defines\n",
    "# the input shape as 48x48 pixels.\n",
    "cnn.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', \n",
    "                 input_shape = (48, 48, 1)))\n",
    "# Adds a max pooling layer with a size of 2x2, which helps mitigate overfitting.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 64 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 128 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattens the data into a 1D vector.\n",
    "cnn.add(Flatten())\n",
    "# Adds a fully connected layer with 128 neurons.\n",
    "cnn.add(Dense(128, activation = 'relu'))\n",
    "# Adds a dropout layer to prevent overfitting. 50% of the inputs are changed\n",
    "# to zero during this training.\n",
    "cnn.add(Dropout(0.5))\n",
    "# Adds an output layer with 7 neurons (1 for each class) and converts the output\n",
    "# into probabilities (softmax). \n",
    "cnn.add(Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "846ad271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the model ready for use with the photos, using the same parameters\n",
    "# as before. \n",
    "cnn.compile(optimizer = Adam(), loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "902805dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.7594 - accuracy: 0.2797 - val_loss: 1.5882 - val_accuracy: 0.3910\n",
      "Epoch 2/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 1.5956 - accuracy: 0.3790 - val_loss: 1.4661 - val_accuracy: 0.4341\n",
      "Epoch 3/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 1.5176 - accuracy: 0.4160 - val_loss: 1.4176 - val_accuracy: 0.4463\n",
      "Epoch 4/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4839 - accuracy: 0.4283 - val_loss: 1.3614 - val_accuracy: 0.4822\n",
      "Epoch 5/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4588 - accuracy: 0.4436 - val_loss: 1.3550 - val_accuracy: 0.4749\n",
      "Epoch 6/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4343 - accuracy: 0.4544 - val_loss: 1.3766 - val_accuracy: 0.4609\n",
      "Epoch 7/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4432 - accuracy: 0.4563 - val_loss: 1.3180 - val_accuracy: 0.5031\n",
      "Epoch 8/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4654 - accuracy: 0.4551 - val_loss: 1.3930 - val_accuracy: 0.4865\n",
      "Epoch 9/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4782 - accuracy: 0.4536 - val_loss: 1.3172 - val_accuracy: 0.5067\n",
      "Epoch 10/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.5001 - accuracy: 0.4470 - val_loss: 1.5222 - val_accuracy: 0.4501\n",
      "Epoch 11/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4882 - accuracy: 0.4568 - val_loss: 1.2891 - val_accuracy: 0.5286\n",
      "Epoch 12/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.5216 - accuracy: 0.4522 - val_loss: 1.3443 - val_accuracy: 0.5136\n",
      "Epoch 13/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.5907 - accuracy: 0.4461 - val_loss: 1.5195 - val_accuracy: 0.4767\n",
      "Epoch 14/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.5962 - accuracy: 0.4491 - val_loss: 1.5338 - val_accuracy: 0.4626\n",
      "Epoch 15/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.6409 - accuracy: 0.4453 - val_loss: 1.5795 - val_accuracy: 0.4842\n",
      "Epoch 16/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.7098 - accuracy: 0.4371 - val_loss: 1.5852 - val_accuracy: 0.4636\n",
      "Epoch 17/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.7502 - accuracy: 0.4351 - val_loss: 1.5085 - val_accuracy: 0.5032\n",
      "Epoch 18/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.8208 - accuracy: 0.4293 - val_loss: 1.5267 - val_accuracy: 0.4763\n",
      "Epoch 19/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.9785 - accuracy: 0.4227 - val_loss: 1.6926 - val_accuracy: 0.4374\n",
      "Epoch 20/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.0427 - accuracy: 0.4212 - val_loss: 1.5415 - val_accuracy: 0.4721\n",
      "Epoch 21/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.0934 - accuracy: 0.4199 - val_loss: 2.0638 - val_accuracy: 0.4060\n",
      "Epoch 22/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.1452 - accuracy: 0.4186 - val_loss: 1.7215 - val_accuracy: 0.4812\n",
      "Epoch 23/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 2.3595 - accuracy: 0.4166 - val_loss: 2.9374 - val_accuracy: 0.4409\n",
      "Epoch 24/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.5694 - accuracy: 0.4044 - val_loss: 2.3087 - val_accuracy: 0.4833\n",
      "Epoch 25/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.7258 - accuracy: 0.4021 - val_loss: 1.9946 - val_accuracy: 0.4738\n",
      "Epoch 26/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.8822 - accuracy: 0.4022 - val_loss: 3.7565 - val_accuracy: 0.4156\n",
      "Epoch 27/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.9423 - accuracy: 0.4054 - val_loss: 2.0128 - val_accuracy: 0.4529\n",
      "Epoch 28/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 3.2722 - accuracy: 0.3970 - val_loss: 3.2414 - val_accuracy: 0.4016\n",
      "Epoch 29/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 3.7575 - accuracy: 0.3959 - val_loss: 2.6246 - val_accuracy: 0.4634\n",
      "Epoch 30/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 3.8339 - accuracy: 0.3989 - val_loss: 4.0095 - val_accuracy: 0.3991\n",
      "Epoch 31/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 4.3903 - accuracy: 0.3949 - val_loss: 3.2956 - val_accuracy: 0.4512\n",
      "Epoch 32/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 4.4545 - accuracy: 0.3935 - val_loss: 3.5923 - val_accuracy: 0.4120\n",
      "Epoch 33/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 5.4107 - accuracy: 0.3928 - val_loss: 4.4295 - val_accuracy: 0.4522\n",
      "Epoch 34/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 6.4753 - accuracy: 0.3870 - val_loss: 8.0922 - val_accuracy: 0.4127\n",
      "Epoch 35/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 7.7243 - accuracy: 0.3895 - val_loss: 7.8019 - val_accuracy: 0.3810\n",
      "Epoch 36/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 7.2292 - accuracy: 0.3895 - val_loss: 6.1277 - val_accuracy: 0.3992\n",
      "Epoch 37/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 8.6719 - accuracy: 0.3895 - val_loss: 6.0209 - val_accuracy: 0.4795\n",
      "Epoch 38/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 10.0728 - accuracy: 0.3829 - val_loss: 6.9995 - val_accuracy: 0.4507\n",
      "Epoch 39/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 11.1941 - accuracy: 0.3874 - val_loss: 10.9804 - val_accuracy: 0.3655\n",
      "Epoch 40/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 10.8785 - accuracy: 0.3885 - val_loss: 10.8188 - val_accuracy: 0.4406\n",
      "Epoch 41/50\n",
      "895/895 [==============================] - 11s 13ms/step - loss: 11.4750 - accuracy: 0.3936 - val_loss: 8.8512 - val_accuracy: 0.4776\n",
      "Epoch 42/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 14.7932 - accuracy: 0.3872 - val_loss: 11.3423 - val_accuracy: 0.4556\n",
      "Epoch 43/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 16.0720 - accuracy: 0.3860 - val_loss: 10.1130 - val_accuracy: 0.4458\n",
      "Epoch 44/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 16.8336 - accuracy: 0.3876 - val_loss: 14.0630 - val_accuracy: 0.3880\n",
      "Epoch 45/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 18.9818 - accuracy: 0.3898 - val_loss: 11.8247 - val_accuracy: 0.4644\n",
      "Epoch 46/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 21.4654 - accuracy: 0.3819 - val_loss: 20.1651 - val_accuracy: 0.4032\n",
      "Epoch 47/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 23.4471 - accuracy: 0.3827 - val_loss: 21.2790 - val_accuracy: 0.3778\n",
      "Epoch 48/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 25.3507 - accuracy: 0.3821 - val_loss: 16.3288 - val_accuracy: 0.4756\n",
      "Epoch 49/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 27.6053 - accuracy: 0.3809 - val_loss: 19.9383 - val_accuracy: 0.4872\n",
      "Epoch 50/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 30.9366 - accuracy: 0.3805 - val_loss: 16.8105 - val_accuracy: 0.4741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x379992dd0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fits the model to the training and validation data, running through the epochs.\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    # Tells the model to divide the number of samples by the batch size of the\n",
    "    # model to calculate how many batches of data will be processed for each epoch.\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size,\n",
    "    # Specifies the number of epochs the model will go through. We set it to\n",
    "    # 50, so the training dataset will go through the model 50 times.\n",
    "    epochs = 50,\n",
    "    # Initializes the validation data the model will use to test its efficiency.\n",
    "    validation_data = test_generator,\n",
    "    # Calculates how many batches of data will be processed for each validation\n",
    "    # epoch. \n",
    "    validation_steps = test_generator.samples // test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "daddbad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/224 [==============================] - 2s 7ms/step - loss: 16.7901 - accuracy: 0.4739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[16.790090560913086, 0.4738972783088684]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the model on the test data.\n",
    "cnn.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b5b45",
   "metadata": {},
   "source": [
    "Adding the parameter of 'zoom_range' in this ninth model seemed to help a bit. The model's accuracy was 47.39%, and the loss value was 16.79. Let's try adding another parameter to see if it gets better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4c58e",
   "metadata": {},
   "source": [
    "###### Model 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4668b4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28645 images belonging to 7 classes.\n",
      "Found 7164 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Almost everything will be the same as the previous model, except we will\n",
    "# add a parameter of 'horizontal_flip' to the 'train_datagenerator' variable.\n",
    "\n",
    "# Initializes the training image data generator.\n",
    "train_datagenerator = ImageDataGenerator(\n",
    "    rescale = 1./255, # Rescales the images by dividing the pixels by 255.\n",
    "    shear_range = 0.2, # Slants the image by up to 20%.\n",
    "    zoom_range = 0.2,  # Zooms into our away from the image by up to 20%.\n",
    "    horizontal_flip = True) # Flips random pictures horizontally.\n",
    "\n",
    "# Initializes the testing image data generator, and also rescales the images\n",
    "# by dividing the pixels by 255, just like the training data.\n",
    "test_datagenerator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Initializes the training generator using the previously defined \n",
    "# 'train_datagenerator' variable to create batches of data from the\n",
    "# training directory.\n",
    "train_generator = train_datagenerator.flow_from_directory(\n",
    "    training,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions.\n",
    "\n",
    "# Initializes the testing generator using the previously defined\n",
    "# 'test_datagenerator' variable to create batches of data from the\n",
    "# testing directory.\n",
    "test_generator = test_datagenerator.flow_from_directory(\n",
    "    testing,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 32, # Defines the batch size (32 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75249340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the Sequential model.\n",
    "cnn = Sequential()\n",
    "\n",
    "# Adds a 2D convolutional layer with 32 filters, all 3x3 pixels, sets the \n",
    "# activation method as ReLU to make all negative values zero, and defines\n",
    "# the input shape as 48x48 pixels.\n",
    "cnn.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', \n",
    "                 input_shape = (48, 48, 1)))\n",
    "# Adds a max pooling layer with a size of 2x2, which helps mitigate overfitting.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 64 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 128 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattens the data into a 1D vector.\n",
    "cnn.add(Flatten())\n",
    "# Adds a fully connected layer with 128 neurons.\n",
    "cnn.add(Dense(128, activation = 'relu'))\n",
    "# Adds a dropout layer to prevent overfitting. 50% of the inputs are changed\n",
    "# to zero during this training.\n",
    "cnn.add(Dropout(0.5))\n",
    "# Adds an output layer with 7 neurons (1 for each class) and converts the output\n",
    "# into probabilities (softmax). \n",
    "cnn.add(Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af84d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the model ready for use with the photos, using the same parameters\n",
    "# as before. \n",
    "cnn.compile(optimizer = Adam(), loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "973c106d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.7572 - accuracy: 0.2846 - val_loss: 1.5647 - val_accuracy: 0.3869\n",
      "Epoch 2/50\n",
      "895/895 [==============================] - 13s 14ms/step - loss: 1.5715 - accuracy: 0.3901 - val_loss: 1.4246 - val_accuracy: 0.4512\n",
      "Epoch 3/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4756 - accuracy: 0.4348 - val_loss: 1.3553 - val_accuracy: 0.4709\n",
      "Epoch 4/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4221 - accuracy: 0.4569 - val_loss: 1.3327 - val_accuracy: 0.4922\n",
      "Epoch 5/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3862 - accuracy: 0.4748 - val_loss: 1.3021 - val_accuracy: 0.5102\n",
      "Epoch 6/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3718 - accuracy: 0.4772 - val_loss: 1.2813 - val_accuracy: 0.5041\n",
      "Epoch 7/50\n",
      "895/895 [==============================] - 13s 14ms/step - loss: 1.3573 - accuracy: 0.4844 - val_loss: 1.2950 - val_accuracy: 0.5150\n",
      "Epoch 8/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3444 - accuracy: 0.4940 - val_loss: 1.2639 - val_accuracy: 0.5237\n",
      "Epoch 9/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3482 - accuracy: 0.4880 - val_loss: 1.2567 - val_accuracy: 0.5282\n",
      "Epoch 10/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3429 - accuracy: 0.4935 - val_loss: 1.2569 - val_accuracy: 0.5265\n",
      "Epoch 11/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3351 - accuracy: 0.4975 - val_loss: 1.2547 - val_accuracy: 0.5233\n",
      "Epoch 12/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3498 - accuracy: 0.4963 - val_loss: 1.2556 - val_accuracy: 0.5301\n",
      "Epoch 13/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3606 - accuracy: 0.4902 - val_loss: 1.2460 - val_accuracy: 0.5406\n",
      "Epoch 14/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3691 - accuracy: 0.4909 - val_loss: 1.2860 - val_accuracy: 0.5164\n",
      "Epoch 15/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3634 - accuracy: 0.4927 - val_loss: 1.2664 - val_accuracy: 0.5198\n",
      "Epoch 16/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3732 - accuracy: 0.4888 - val_loss: 1.2324 - val_accuracy: 0.5335\n",
      "Epoch 17/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 1.3861 - accuracy: 0.4890 - val_loss: 1.3229 - val_accuracy: 0.5249\n",
      "Epoch 18/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4078 - accuracy: 0.4875 - val_loss: 1.2751 - val_accuracy: 0.5220\n",
      "Epoch 19/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.3987 - accuracy: 0.4901 - val_loss: 1.3321 - val_accuracy: 0.4997\n",
      "Epoch 20/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4068 - accuracy: 0.4901 - val_loss: 1.3261 - val_accuracy: 0.5343\n",
      "Epoch 21/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4383 - accuracy: 0.4792 - val_loss: 1.2943 - val_accuracy: 0.5146\n",
      "Epoch 22/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4433 - accuracy: 0.4826 - val_loss: 1.3360 - val_accuracy: 0.5109\n",
      "Epoch 23/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.4488 - accuracy: 0.4826 - val_loss: 1.3466 - val_accuracy: 0.5140\n",
      "Epoch 24/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 1.4653 - accuracy: 0.4797 - val_loss: 1.4300 - val_accuracy: 0.5025\n",
      "Epoch 25/50\n",
      "895/895 [==============================] - 14s 16ms/step - loss: 1.4644 - accuracy: 0.4809 - val_loss: 1.4219 - val_accuracy: 0.5006\n",
      "Epoch 26/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 1.5058 - accuracy: 0.4757 - val_loss: 1.4608 - val_accuracy: 0.4917\n",
      "Epoch 27/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 1.5509 - accuracy: 0.4732 - val_loss: 1.2354 - val_accuracy: 0.5411\n",
      "Epoch 28/50\n",
      "895/895 [==============================] - 14s 15ms/step - loss: 1.5535 - accuracy: 0.4722 - val_loss: 1.8422 - val_accuracy: 0.4008\n",
      "Epoch 29/50\n",
      "895/895 [==============================] - 13s 15ms/step - loss: 1.5803 - accuracy: 0.4674 - val_loss: 1.6524 - val_accuracy: 0.4793\n",
      "Epoch 30/50\n",
      "895/895 [==============================] - 13s 15ms/step - loss: 1.5699 - accuracy: 0.4733 - val_loss: 1.5311 - val_accuracy: 0.4683\n",
      "Epoch 31/50\n",
      "895/895 [==============================] - 14s 16ms/step - loss: 1.5921 - accuracy: 0.4714 - val_loss: 1.4744 - val_accuracy: 0.4851\n",
      "Epoch 32/50\n",
      "895/895 [==============================] - 14s 16ms/step - loss: 1.6762 - accuracy: 0.4630 - val_loss: 1.6350 - val_accuracy: 0.4563\n",
      "Epoch 33/50\n",
      "895/895 [==============================] - 16s 17ms/step - loss: 1.6743 - accuracy: 0.4680 - val_loss: 1.4466 - val_accuracy: 0.5142\n",
      "Epoch 34/50\n",
      "895/895 [==============================] - 15s 17ms/step - loss: 1.6797 - accuracy: 0.4611 - val_loss: 1.5277 - val_accuracy: 0.5146\n",
      "Epoch 35/50\n",
      "895/895 [==============================] - 15s 16ms/step - loss: 1.7090 - accuracy: 0.4639 - val_loss: 1.2870 - val_accuracy: 0.5411\n",
      "Epoch 36/50\n",
      "895/895 [==============================] - 15s 16ms/step - loss: 1.6845 - accuracy: 0.4649 - val_loss: 1.3670 - val_accuracy: 0.4902\n",
      "Epoch 37/50\n",
      "895/895 [==============================] - 14s 16ms/step - loss: 1.7389 - accuracy: 0.4602 - val_loss: 1.5216 - val_accuracy: 0.5102\n",
      "Epoch 38/50\n",
      "895/895 [==============================] - 14s 16ms/step - loss: 1.8713 - accuracy: 0.4541 - val_loss: 1.5022 - val_accuracy: 0.4875\n",
      "Epoch 39/50\n",
      "895/895 [==============================] - 15s 16ms/step - loss: 1.8606 - accuracy: 0.4567 - val_loss: 1.4942 - val_accuracy: 0.5022\n",
      "Epoch 40/50\n",
      "895/895 [==============================] - 14s 16ms/step - loss: 1.9486 - accuracy: 0.4523 - val_loss: 1.6758 - val_accuracy: 0.5094\n",
      "Epoch 41/50\n",
      "895/895 [==============================] - 14s 15ms/step - loss: 2.0133 - accuracy: 0.4524 - val_loss: 1.6458 - val_accuracy: 0.5198\n",
      "Epoch 42/50\n",
      "895/895 [==============================] - 14s 15ms/step - loss: 2.0520 - accuracy: 0.4507 - val_loss: 1.4228 - val_accuracy: 0.5315\n",
      "Epoch 43/50\n",
      "895/895 [==============================] - 13s 14ms/step - loss: 2.0554 - accuracy: 0.4508 - val_loss: 1.8211 - val_accuracy: 0.4818\n",
      "Epoch 44/50\n",
      "895/895 [==============================] - 13s 14ms/step - loss: 2.1699 - accuracy: 0.4462 - val_loss: 2.0196 - val_accuracy: 0.4898\n",
      "Epoch 45/50\n",
      "895/895 [==============================] - 13s 14ms/step - loss: 2.2492 - accuracy: 0.4421 - val_loss: 2.3343 - val_accuracy: 0.4065\n",
      "Epoch 46/50\n",
      "895/895 [==============================] - 12s 13ms/step - loss: 2.2580 - accuracy: 0.4493 - val_loss: 2.5632 - val_accuracy: 0.4889\n",
      "Epoch 47/50\n",
      "895/895 [==============================] - 13s 14ms/step - loss: 2.4734 - accuracy: 0.4433 - val_loss: 1.8324 - val_accuracy: 0.4871\n",
      "Epoch 48/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 2.2772 - accuracy: 0.4442 - val_loss: 1.8627 - val_accuracy: 0.4811\n",
      "Epoch 49/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 2.7648 - accuracy: 0.4341 - val_loss: 2.0044 - val_accuracy: 0.5007\n",
      "Epoch 50/50\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 2.5512 - accuracy: 0.4410 - val_loss: 1.7361 - val_accuracy: 0.5121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x379992d90>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fits the model to the training and validation data, running through the epochs.\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    # Tells the model to divide the number of samples by the batch size of the\n",
    "    # model to calculate how many batches of data will be processed for each epoch.\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size,\n",
    "    # Specifies the number of epochs the model will go through. We set it to\n",
    "    # 50, so the training dataset will go through the model 50 times.\n",
    "    epochs = 50,\n",
    "    # Initializes the validation data the model will use to test its efficiency.\n",
    "    validation_data = test_generator,\n",
    "    # Calculates how many batches of data will be processed for each validation\n",
    "    # epoch. \n",
    "    validation_steps = test_generator.samples // test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad5bb246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/224 [==============================] - 2s 7ms/step - loss: 1.7359 - accuracy: 0.5113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7359355688095093, 0.5113065242767334]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the model on the test data.\n",
    "cnn.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c44d943",
   "metadata": {},
   "source": [
    "Adding the parameter of 'horizontal_flip' in this tenth model seemed to help quite a bit. The model's accuracy was 51.13%, and the loss value was 1.74. Let's try adding another parameter to see if it gets better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3b7fd",
   "metadata": {},
   "source": [
    "###### Model 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ea4db8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28645 images belonging to 7 classes.\n",
      "Found 7164 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Almost everything will be the same as the fourth model, except we will\n",
    "# make the batch size bigger for the train and test generators.\n",
    "\n",
    "# Initializes the training image data generator.\n",
    "train_datagenerator = ImageDataGenerator(\n",
    "    rescale = 1./255, # Rescales the images by dividing the pixels by 255.\n",
    "    shear_range = 0.2, # Slants the image by up to 20%.\n",
    "    zoom_range = 0.2,  # Zooms into our away from the image by up to 20%.\n",
    "    horizontal_flip = True) # Flips random pictures horizontally.\n",
    "\n",
    "# Initializes the testing image data generator, and also rescales the images\n",
    "# by dividing the pixels by 255, just like the training data.\n",
    "test_datagenerator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Initializes the training generator using the previously defined \n",
    "# 'train_datagenerator' variable to create batches of data from the\n",
    "# training directory.\n",
    "train_generator = train_datagenerator.flow_from_directory(\n",
    "    training,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 64, # Defines the batch size (64 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions.\n",
    "\n",
    "# Initializes the testing generator using the previously defined\n",
    "# 'test_datagenerator' variable to create batches of data from the\n",
    "# testing directory.\n",
    "test_generator = test_datagenerator.flow_from_directory(\n",
    "    testing,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 64, # Defines the batch size (64 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc3a452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the Sequential model.\n",
    "cnn = Sequential()\n",
    "\n",
    "# Adds a 2D convolutional layer with 32 filters, all 3x3 pixels, sets the \n",
    "# activation method as ReLU to make all negative values zero, and defines\n",
    "# the input shape as 48x48 pixels.\n",
    "cnn.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', \n",
    "                 input_shape = (48, 48, 1)))\n",
    "# Adds a max pooling layer with a size of 2x2, which helps mitigate overfitting.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 64 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 128 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattens the data into a 1D vector.\n",
    "cnn.add(Flatten())\n",
    "# Adds a fully connected layer with 128 neurons.\n",
    "cnn.add(Dense(128, activation = 'relu'))\n",
    "# Adds a dropout layer to prevent overfitting. 50% of the inputs are changed\n",
    "# to zero during this training.\n",
    "cnn.add(Dropout(0.5))\n",
    "# Adds an output layer with 7 neurons (1 for each class) and converts the output\n",
    "# into probabilities (softmax). \n",
    "cnn.add(Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77051181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the model ready for use with the photos, using the same parameters\n",
    "# as before. \n",
    "cnn.compile(optimizer = Adam(), loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c78b5e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "447/447 [==============================] - 10s 22ms/step - loss: 1.7844 - accuracy: 0.2708 - val_loss: 1.6111 - val_accuracy: 0.3774\n",
      "Epoch 2/50\n",
      "447/447 [==============================] - 11s 24ms/step - loss: 1.6235 - accuracy: 0.3698 - val_loss: 1.4673 - val_accuracy: 0.4416\n",
      "Epoch 3/50\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 1.5355 - accuracy: 0.4102 - val_loss: 1.4011 - val_accuracy: 0.4631\n",
      "Epoch 4/50\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 1.4782 - accuracy: 0.4348 - val_loss: 1.3631 - val_accuracy: 0.4803\n",
      "Epoch 5/50\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 1.4532 - accuracy: 0.4459 - val_loss: 1.3240 - val_accuracy: 0.4955\n",
      "Epoch 6/50\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 1.4238 - accuracy: 0.4551 - val_loss: 1.3261 - val_accuracy: 0.5010\n",
      "Epoch 7/50\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 1.4040 - accuracy: 0.4640 - val_loss: 1.2832 - val_accuracy: 0.5030\n",
      "Epoch 8/50\n",
      "447/447 [==============================] - 10s 21ms/step - loss: 1.3997 - accuracy: 0.4630 - val_loss: 1.2686 - val_accuracy: 0.5103\n",
      "Epoch 9/50\n",
      "447/447 [==============================] - 10s 22ms/step - loss: 1.3887 - accuracy: 0.4734 - val_loss: 1.2980 - val_accuracy: 0.5108\n",
      "Epoch 10/50\n",
      "447/447 [==============================] - 10s 22ms/step - loss: 1.3816 - accuracy: 0.4768 - val_loss: 1.2656 - val_accuracy: 0.5176\n",
      "Epoch 11/50\n",
      "447/447 [==============================] - 11s 24ms/step - loss: 1.3800 - accuracy: 0.4783 - val_loss: 1.2892 - val_accuracy: 0.5065\n",
      "Epoch 12/50\n",
      "447/447 [==============================] - 10s 22ms/step - loss: 1.3712 - accuracy: 0.4824 - val_loss: 1.2570 - val_accuracy: 0.5193\n",
      "Epoch 13/50\n",
      "447/447 [==============================] - 10s 22ms/step - loss: 1.3714 - accuracy: 0.4800 - val_loss: 1.2495 - val_accuracy: 0.5286\n",
      "Epoch 14/50\n",
      "447/447 [==============================] - 12s 26ms/step - loss: 1.3640 - accuracy: 0.4853 - val_loss: 1.2822 - val_accuracy: 0.5076\n",
      "Epoch 15/50\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 1.3790 - accuracy: 0.4821 - val_loss: 1.2579 - val_accuracy: 0.5258\n",
      "Epoch 16/50\n",
      "447/447 [==============================] - 10s 22ms/step - loss: 1.3830 - accuracy: 0.4800 - val_loss: 1.3180 - val_accuracy: 0.4931\n",
      "Epoch 17/50\n",
      "447/447 [==============================] - 10s 22ms/step - loss: 1.3753 - accuracy: 0.4841 - val_loss: 1.3037 - val_accuracy: 0.5031\n",
      "Epoch 18/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 1.3797 - accuracy: 0.4839 - val_loss: 1.2696 - val_accuracy: 0.5145\n",
      "Epoch 19/50\n",
      "447/447 [==============================] - 11s 24ms/step - loss: 1.3870 - accuracy: 0.4841 - val_loss: 1.2298 - val_accuracy: 0.5277\n",
      "Epoch 20/50\n",
      "447/447 [==============================] - 10s 22ms/step - loss: 1.3755 - accuracy: 0.4876 - val_loss: 1.2512 - val_accuracy: 0.5189\n",
      "Epoch 21/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.3778 - accuracy: 0.4874 - val_loss: 1.2784 - val_accuracy: 0.5183\n",
      "Epoch 22/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.3985 - accuracy: 0.4832 - val_loss: 1.2900 - val_accuracy: 0.5117\n",
      "Epoch 23/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.3856 - accuracy: 0.4877 - val_loss: 1.3015 - val_accuracy: 0.5138\n",
      "Epoch 24/50\n",
      "447/447 [==============================] - 9s 19ms/step - loss: 1.4034 - accuracy: 0.4842 - val_loss: 1.2787 - val_accuracy: 0.5239\n",
      "Epoch 25/50\n",
      "447/447 [==============================] - 8s 19ms/step - loss: 1.4026 - accuracy: 0.4859 - val_loss: 1.3170 - val_accuracy: 0.5234\n",
      "Epoch 26/50\n",
      "447/447 [==============================] - 8s 19ms/step - loss: 1.4241 - accuracy: 0.4796 - val_loss: 1.2864 - val_accuracy: 0.5131\n",
      "Epoch 27/50\n",
      "447/447 [==============================] - 8s 19ms/step - loss: 1.4328 - accuracy: 0.4818 - val_loss: 1.2184 - val_accuracy: 0.5443\n",
      "Epoch 28/50\n",
      "447/447 [==============================] - 8s 18ms/step - loss: 1.4177 - accuracy: 0.4828 - val_loss: 1.2876 - val_accuracy: 0.5193\n",
      "Epoch 29/50\n",
      "447/447 [==============================] - 8s 18ms/step - loss: 1.4442 - accuracy: 0.4781 - val_loss: 1.2817 - val_accuracy: 0.5198\n",
      "Epoch 30/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.4541 - accuracy: 0.4768 - val_loss: 1.2980 - val_accuracy: 0.5177\n",
      "Epoch 31/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.4448 - accuracy: 0.4816 - val_loss: 1.2535 - val_accuracy: 0.5366\n",
      "Epoch 32/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.4559 - accuracy: 0.4776 - val_loss: 1.2189 - val_accuracy: 0.5436\n",
      "Epoch 33/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.4789 - accuracy: 0.4736 - val_loss: 1.2579 - val_accuracy: 0.5390\n",
      "Epoch 34/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.4845 - accuracy: 0.4737 - val_loss: 1.4731 - val_accuracy: 0.4744\n",
      "Epoch 35/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.4914 - accuracy: 0.4716 - val_loss: 1.3042 - val_accuracy: 0.5204\n",
      "Epoch 36/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.5166 - accuracy: 0.4723 - val_loss: 1.3470 - val_accuracy: 0.5056\n",
      "Epoch 37/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.5122 - accuracy: 0.4764 - val_loss: 1.4102 - val_accuracy: 0.4934\n",
      "Epoch 38/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.5390 - accuracy: 0.4728 - val_loss: 1.3451 - val_accuracy: 0.5315\n",
      "Epoch 39/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.5473 - accuracy: 0.4738 - val_loss: 1.4336 - val_accuracy: 0.4963\n",
      "Epoch 40/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.5767 - accuracy: 0.4689 - val_loss: 1.5910 - val_accuracy: 0.5042\n",
      "Epoch 41/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.5722 - accuracy: 0.4671 - val_loss: 1.6476 - val_accuracy: 0.4476\n",
      "Epoch 42/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.6185 - accuracy: 0.4633 - val_loss: 1.5339 - val_accuracy: 0.4790\n",
      "Epoch 43/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.6447 - accuracy: 0.4625 - val_loss: 1.2636 - val_accuracy: 0.5501\n",
      "Epoch 44/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.6235 - accuracy: 0.4661 - val_loss: 1.3868 - val_accuracy: 0.5220\n",
      "Epoch 45/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.6465 - accuracy: 0.4583 - val_loss: 1.4090 - val_accuracy: 0.5096\n",
      "Epoch 46/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.6542 - accuracy: 0.4651 - val_loss: 1.4338 - val_accuracy: 0.5101\n",
      "Epoch 47/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.6200 - accuracy: 0.4676 - val_loss: 1.6083 - val_accuracy: 0.4668\n",
      "Epoch 48/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.6862 - accuracy: 0.4575 - val_loss: 1.3494 - val_accuracy: 0.5120\n",
      "Epoch 49/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.7066 - accuracy: 0.4618 - val_loss: 1.2843 - val_accuracy: 0.5404\n",
      "Epoch 50/50\n",
      "447/447 [==============================] - 8s 17ms/step - loss: 1.6994 - accuracy: 0.4638 - val_loss: 1.3029 - val_accuracy: 0.5522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x37fd919d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fits the model to the training and validation data, running through the epochs.\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    # Tells the model to divide the number of samples by the batch size of the\n",
    "    # model to calculate how many batches of data will be processed for each epoch.\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size,\n",
    "    # Specifies the number of epochs the model will go through. We set it to\n",
    "    # 50, so the training dataset will go through the model 50 times.\n",
    "    epochs = 50,\n",
    "    # Initializes the validation data the model will use to test its efficiency.\n",
    "    validation_data = test_generator,\n",
    "    # Calculates how many batches of data will be processed for each validation\n",
    "    # epoch. \n",
    "    validation_steps = test_generator.samples // test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86956494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 1s 8ms/step - loss: 1.3033 - accuracy: 0.5518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.303282618522644, 0.5517867207527161]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the model on the test data.\n",
    "cnn.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cce82",
   "metadata": {},
   "source": [
    "Including a larger batch size in this eleventh model seemed to help even more. The model's accuracy was 55.18%, and the loss value was 1.30. Let's try adding a batch normalization function in the model set-up to see if it gets even better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad40f00",
   "metadata": {},
   "source": [
    "###### Model 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5bd0029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28645 images belonging to 7 classes.\n",
      "Found 7164 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Everything here will be the same as the previous model.\n",
    "\n",
    "# Initializes the training image data generator.\n",
    "train_datagenerator = ImageDataGenerator(\n",
    "    rescale = 1./255, # Rescales the images by dividing the pixels by 255.\n",
    "    shear_range = 0.2, # Slants the image by up to 20%.\n",
    "    zoom_range = 0.2,  # Zooms into our away from the image by up to 20%.\n",
    "    horizontal_flip = True) # Flips random pictures horizontally.\n",
    "\n",
    "# Initializes the testing image data generator, and also rescales the images\n",
    "# by dividing the pixels by 255, just like the training data.\n",
    "test_datagenerator = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Initializes the training generator using the previously defined \n",
    "# 'train_datagenerator' variable to create batches of data from the\n",
    "# training directory.\n",
    "train_generator = train_datagenerator.flow_from_directory(\n",
    "    training,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 64, # Defines the batch size (64 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions.\n",
    "\n",
    "# Initializes the testing generator using the previously defined\n",
    "# 'test_datagenerator' variable to create batches of data from the\n",
    "# testing directory.\n",
    "test_generator = test_datagenerator.flow_from_directory(\n",
    "    testing,\n",
    "    target_size = (48, 48), # Defines the size of the photos in pixels.\n",
    "    batch_size = 64, # Defines the batch size (64 photos per batch).\n",
    "    color_mode = 'grayscale', # Defines the color of the photos (grayscale).\n",
    "    class_mode = 'categorical') # Defines the class mode as categorical since the\n",
    "                                # data will be classified into different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fec62d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will add Batch Normalization here to see how it impacts the model.\n",
    "\n",
    "# Initializes the Sequential model.\n",
    "cnn = Sequential()\n",
    "\n",
    "# Adds a 2D convolutional layer with 32 filters, all 3x3 pixels, sets the \n",
    "# activation method as ReLU to make all negative values zero, and defines\n",
    "# the input shape as 48x48 pixels.\n",
    "cnn.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', \n",
    "                 input_shape = (48, 48, 1)))\n",
    "cnn.add(BatchNormalization()) # Adds Batch Normalization to the model.\n",
    "# Adds a max pooling layer with a size of 2x2, which helps mitigate overfitting.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 64 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(BatchNormalization()) # Adds Batch Normalization to the model.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adds a convolutional layer with 128 filters, all 3x3 pixels again.\n",
    "cnn.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "cnn.add(BatchNormalization()) # Adds Batch Normalization to the model.\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattens the data into a 1D vector.\n",
    "cnn.add(Flatten())\n",
    "# Adds a fully connected layer with 128 neurons.\n",
    "cnn.add(Dense(128, activation = 'relu'))\n",
    "# Adds a dropout layer to prevent overfitting. 50% of the inputs are changed\n",
    "# to zero during this training.\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(BatchNormalization()) # Adds Batch Normalization to the model.\n",
    "# Adds an output layer with 7 neurons (1 for each class) and converts the output\n",
    "# into probabilities (softmax). \n",
    "cnn.add(Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8efd97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the model ready for use with the photos, using the same parameters\n",
    "# as before. \n",
    "cnn.compile(optimizer = Adam(), loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f97f000d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "447/447 [==============================] - 10s 21ms/step - loss: 1.7646 - accuracy: 0.3389 - val_loss: 1.5992 - val_accuracy: 0.3898\n",
      "Epoch 2/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.4482 - accuracy: 0.4495 - val_loss: 1.4964 - val_accuracy: 0.4174\n",
      "Epoch 3/50\n",
      "447/447 [==============================] - 10s 22ms/step - loss: 1.3447 - accuracy: 0.4913 - val_loss: 1.2948 - val_accuracy: 0.5113\n",
      "Epoch 4/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 1.2838 - accuracy: 0.5134 - val_loss: 1.2286 - val_accuracy: 0.5351\n",
      "Epoch 5/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.2435 - accuracy: 0.5322 - val_loss: 1.2654 - val_accuracy: 0.5168\n",
      "Epoch 6/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.2119 - accuracy: 0.5442 - val_loss: 1.2181 - val_accuracy: 0.5429\n",
      "Epoch 7/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.1851 - accuracy: 0.5553 - val_loss: 1.2085 - val_accuracy: 0.5339\n",
      "Epoch 8/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.1598 - accuracy: 0.5671 - val_loss: 1.4665 - val_accuracy: 0.4668\n",
      "Epoch 9/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.1423 - accuracy: 0.5724 - val_loss: 1.1981 - val_accuracy: 0.5532\n",
      "Epoch 10/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.1312 - accuracy: 0.5767 - val_loss: 1.1724 - val_accuracy: 0.5576\n",
      "Epoch 11/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 1.1118 - accuracy: 0.5857 - val_loss: 1.1392 - val_accuracy: 0.5757\n",
      "Epoch 12/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 1.0947 - accuracy: 0.5942 - val_loss: 1.1381 - val_accuracy: 0.5763\n",
      "Epoch 13/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 1.0873 - accuracy: 0.5929 - val_loss: 1.1053 - val_accuracy: 0.5874\n",
      "Epoch 14/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.0775 - accuracy: 0.5997 - val_loss: 1.1611 - val_accuracy: 0.5663\n",
      "Epoch 15/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.0661 - accuracy: 0.6038 - val_loss: 1.1931 - val_accuracy: 0.5579\n",
      "Epoch 16/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.0544 - accuracy: 0.6062 - val_loss: 1.1934 - val_accuracy: 0.5583\n",
      "Epoch 17/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 1.0491 - accuracy: 0.6102 - val_loss: 1.1758 - val_accuracy: 0.5588\n",
      "Epoch 18/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 1.0344 - accuracy: 0.6171 - val_loss: 1.1238 - val_accuracy: 0.5843\n",
      "Epoch 19/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 1.0296 - accuracy: 0.6187 - val_loss: 1.3139 - val_accuracy: 0.5184\n",
      "Epoch 20/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.0246 - accuracy: 0.6198 - val_loss: 1.0787 - val_accuracy: 0.6052\n",
      "Epoch 21/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 1.0082 - accuracy: 0.6291 - val_loss: 1.0804 - val_accuracy: 0.5953\n",
      "Epoch 22/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 1.0071 - accuracy: 0.6279 - val_loss: 1.1083 - val_accuracy: 0.5828\n",
      "Epoch 23/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.9965 - accuracy: 0.6314 - val_loss: 1.1022 - val_accuracy: 0.5921\n",
      "Epoch 24/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.9921 - accuracy: 0.6305 - val_loss: 1.1426 - val_accuracy: 0.5812\n",
      "Epoch 25/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.9851 - accuracy: 0.6341 - val_loss: 1.1129 - val_accuracy: 0.5847\n",
      "Epoch 26/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.9811 - accuracy: 0.6381 - val_loss: 1.1884 - val_accuracy: 0.5683\n",
      "Epoch 27/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.9769 - accuracy: 0.6387 - val_loss: 1.1212 - val_accuracy: 0.5825\n",
      "Epoch 28/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.9700 - accuracy: 0.6441 - val_loss: 1.1061 - val_accuracy: 0.5959\n",
      "Epoch 29/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.9645 - accuracy: 0.6446 - val_loss: 1.1844 - val_accuracy: 0.5657\n",
      "Epoch 30/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.9635 - accuracy: 0.6445 - val_loss: 1.1041 - val_accuracy: 0.5907\n",
      "Epoch 31/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.9535 - accuracy: 0.6489 - val_loss: 1.0633 - val_accuracy: 0.6091\n",
      "Epoch 32/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.9509 - accuracy: 0.6511 - val_loss: 1.0863 - val_accuracy: 0.5971\n",
      "Epoch 33/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.9455 - accuracy: 0.6503 - val_loss: 1.1089 - val_accuracy: 0.5904\n",
      "Epoch 34/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.9491 - accuracy: 0.6526 - val_loss: 1.0934 - val_accuracy: 0.5960\n",
      "Epoch 35/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.9356 - accuracy: 0.6544 - val_loss: 1.0797 - val_accuracy: 0.6059\n",
      "Epoch 36/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.9292 - accuracy: 0.6602 - val_loss: 1.1110 - val_accuracy: 0.5939\n",
      "Epoch 37/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.9247 - accuracy: 0.6603 - val_loss: 1.1917 - val_accuracy: 0.5587\n",
      "Epoch 38/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.9280 - accuracy: 0.6615 - val_loss: 1.0692 - val_accuracy: 0.6099\n",
      "Epoch 39/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.9208 - accuracy: 0.6617 - val_loss: 1.1284 - val_accuracy: 0.5936\n",
      "Epoch 40/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.9167 - accuracy: 0.6615 - val_loss: 1.0798 - val_accuracy: 0.6018\n",
      "Epoch 41/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.9118 - accuracy: 0.6675 - val_loss: 1.2270 - val_accuracy: 0.5559\n",
      "Epoch 42/50\n",
      "447/447 [==============================] - 10s 21ms/step - loss: 0.9100 - accuracy: 0.6696 - val_loss: 1.1034 - val_accuracy: 0.5978\n",
      "Epoch 43/50\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.9088 - accuracy: 0.6692 - val_loss: 1.2124 - val_accuracy: 0.5591\n",
      "Epoch 44/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.9011 - accuracy: 0.6706 - val_loss: 1.1250 - val_accuracy: 0.5911\n",
      "Epoch 45/50\n",
      "447/447 [==============================] - 10s 21ms/step - loss: 0.9055 - accuracy: 0.6710 - val_loss: 1.0834 - val_accuracy: 0.6042\n",
      "Epoch 46/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.9005 - accuracy: 0.6690 - val_loss: 1.0905 - val_accuracy: 0.5990\n",
      "Epoch 47/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.9015 - accuracy: 0.6723 - val_loss: 1.0833 - val_accuracy: 0.6050\n",
      "Epoch 48/50\n",
      "447/447 [==============================] - 10s 22ms/step - loss: 0.8953 - accuracy: 0.6751 - val_loss: 1.1274 - val_accuracy: 0.5892\n",
      "Epoch 49/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.8878 - accuracy: 0.6733 - val_loss: 1.1066 - val_accuracy: 0.6002\n",
      "Epoch 50/50\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.8838 - accuracy: 0.6786 - val_loss: 1.0845 - val_accuracy: 0.6070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1760c40d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fits the model to the training and validation data, running through the epochs.\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    # Tells the model to divide the number of samples by the batch size of the\n",
    "    # model to calculate how many batches of data will be processed for each epoch.\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size,\n",
    "    # Specifies the number of epochs the model will go through. We set it to\n",
    "    # 50, so the training dataset will go through the model 50 times.\n",
    "    epochs = 50,\n",
    "    # Initializes the validation data the model will use to test its efficiency.\n",
    "    validation_data = test_generator,\n",
    "    # Calculates how many batches of data will be processed for each validation\n",
    "    # epoch. \n",
    "    validation_steps = test_generator.samples // test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "117f5ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 1s 8ms/step - loss: 1.0873 - accuracy: 0.6059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0873451232910156, 0.60594642162323]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the model on the test data.\n",
    "cnn.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8061a4ad",
   "metadata": {},
   "source": [
    "Adding a batch normalization function in this twelveth model seemed to improve the model a lot. The model's accuracy was 60.59%, and the loss value was 1.09."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9538c1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 1s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.15      0.15      0.15       956\n",
      "     disgust       0.00      0.00      0.00       111\n",
      "        fear       0.15      0.18      0.16      1023\n",
      "       happy       0.25      0.23      0.24      1772\n",
      "     neutral       0.18      0.18      0.18      1228\n",
      "         sad       0.16      0.16      0.16      1245\n",
      "    surprise       0.11      0.11      0.11       829\n",
      "\n",
      "    accuracy                           0.17      7164\n",
      "   macro avg       0.14      0.14      0.14      7164\n",
      "weighted avg       0.17      0.17      0.17      7164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculates the predicted values on the test data and stores it in a variable.\n",
    "# The '+ 1' is to make sure any remaining photos that aren't included in a \n",
    "# full batch are included in the predictions.\n",
    "Y_preds = cnn.predict(test_generator, \n",
    "                       test_generator.samples // test_generator.batch_size + 1)\n",
    "# Converts the probabilities to class labels. \n",
    "preds = np.argmax(Y_preds, axis=1)\n",
    "# Retrieves the true class labels for the images.\n",
    "true_vals = test_generator.classes\n",
    "# Retrieves the labels from the 'test_generator' variable.\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "# Generates and prints the classification report of the predicted vs. \n",
    "# true classes.\n",
    "print(classification_report(true_vals, preds, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "677b9179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creates a confusion matrix of the true vs. predicted classes.\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plots the confusion matrix using seaborn, and matplotlib is used to\n",
    "# display it.\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce84bc",
   "metadata": {},
   "source": [
    "##### References\n",
    "- https://www.tensorflow.org/tutorials/images/cnn\n",
    "- https://www.analyticsvidhya.com/blog/2021/01/image-classification-using-convolutional-neural-networks-a-step-by-step-guide/\n",
    "- https://www.geeksforgeeks.org/emotion-detection-using-convolutional-neural-networks-cnns/\n",
    "- https://medium.com/@birdortyedi_23820/deep-learning-lab-episode-3-fer2013-c38f2e052280#:~:text=fer2013%20is%20an%20open%2Dsource,competition%2C%20shortly%20before%20ICML%202013\n",
    "- https://nupmanyu.medium.com/understanding-cnn-convolution-neural-network-b6eebab447f2\n",
    "- https://www.baeldung.com/cs/batch-normalization-cnn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
